{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets.openml_datasets import OpenML_Dataset\n",
    "from datasets.torch_datasets import Torch_Dataset\n",
    "from utils.weakener import Weakener\n",
    "from models.general_model import MLP\n",
    "from utils.losses import PartialLoss,LBLoss,EMLoss,OSLCELoss,OSLBrierLoss,CELoss\n",
    "from utils.trainig_testing import train_model,evaluate_model,train_and_evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "Data = Torch_Dataset('mnist', batch_size=16)\n",
    "Weak = Weakener(Data.num_classes)\n",
    "#Weak.generate_M(model_class='pll')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.        , 0.        , 0.00217014, 0.00217014],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.        , 0.00217014, 0.        , 0.00217014],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.        , 0.00217014, 0.00217014, 0.        ],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.        , 0.00195312, 0.00195312, 0.00195312],\n        [0.        , 0.        , 0.        , 0.        , 0.        ,\n         0.        , 0.00217014, 0.        , 0.        , 0.00217014]]),\n array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n        [0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n        [0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n        [0, 0, 0, 0, 0, 0, 1, 0, 0, 1]]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weak.generate_M('pll',pll_p=0.5)\n",
    "Weak.M[:5,:],Weak.Z[:5,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([717, 934, 166,  ..., 998,  98,  71], dtype=torch.int32),\n tensor([[1., 0., 1.,  ..., 0., 0., 0.],\n         [1., 1., 1.,  ..., 0., 0., 1.],\n         [0., 0., 1.,  ..., 1., 1., 1.],\n         ...,\n         [1., 1., 1.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 0., 1., 0.],\n         [0., 0., 0.,  ..., 1., 1., 1.]], dtype=torch.float64))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X,train_y,test_X,test_y =  Data.get_data()\n",
    "print(train_X.shape)\n",
    "Weak.generate_weak(train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n tensor([717, 934, 166, 457, 584], dtype=torch.int32),\n tensor([[1., 0., 1., 1., 0., 1., 1., 0., 0., 0.],\n         [1., 1., 1., 0., 1., 1., 0., 0., 0., 1.],\n         [0., 0., 1., 0., 1., 0., 1., 1., 1., 1.],\n         [0., 1., 1., 1., 0., 1., 0., 0., 1., 1.],\n         [1., 0., 0., 1., 0., 1., 0., 0., 1., 1.]], dtype=torch.float64))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:5,:],Weak.z[:5],Weak.w[:5,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "mlp_feature = MLP(Data.num_features, [Data.num_features, Data.num_features, Data.num_features], Data.num_classes, dropout_p = 0.0, bn = False, seed = 1,\n",
    "                  layer_init = lambda x: nn.init.kaiming_uniform_(x, a=math.sqrt(5)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class mlp_feature(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(mlp_feature, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(x.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        feature = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return feature, out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "mlp_feat = mlp_feature(Data.num_features, Data.num_features, Data.num_classes)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('fc1.weight',\n              tensor([[ 0.0160,  0.0357,  0.0259,  ...,  0.0346,  0.0098, -0.0007],\n                      [-0.0038,  0.0186, -0.0336,  ..., -0.0220, -0.0235, -0.0181],\n                      [ 0.0314,  0.0101, -0.0083,  ...,  0.0088,  0.0324,  0.0314],\n                      ...,\n                      [-0.0005, -0.0201, -0.0312,  ...,  0.0286,  0.0098, -0.0282],\n                      [-0.0321, -0.0128, -0.0284,  ..., -0.0145,  0.0013, -0.0034],\n                      [ 0.0264, -0.0247,  0.0258,  ...,  0.0247,  0.0001,  0.0357]])),\n             ('fc1.bias',\n              tensor([-2.5243e-02,  3.1946e-02, -2.4145e-02, -1.5259e-02,  1.0686e-02,\n                      -7.4537e-03, -3.3765e-02, -3.4047e-02,  3.6584e-03, -1.4164e-02,\n                      -9.2037e-03,  9.8986e-03, -1.8367e-02,  1.4549e-02, -2.0212e-02,\n                       1.5605e-02, -2.8576e-02,  1.5315e-02, -2.2751e-02,  1.5439e-02,\n                      -1.0001e-02,  3.1898e-02,  3.3839e-02, -1.1089e-02,  1.5177e-02,\n                       1.0267e-02, -1.7218e-02, -1.3150e-02, -2.3162e-02,  2.1205e-02,\n                       2.8839e-02,  1.0188e-02, -2.1783e-02, -2.4310e-02, -1.8701e-02,\n                      -2.0048e-02, -2.4024e-02,  2.9793e-02,  2.0797e-02, -2.9951e-02,\n                      -1.5352e-02,  3.3379e-02, -9.1811e-03, -2.9347e-03,  7.8059e-03,\n                       8.0239e-03, -1.9163e-02, -3.3531e-02, -2.6548e-02, -2.2794e-02,\n                      -3.2796e-02,  1.5848e-03, -7.0128e-03,  1.1314e-02,  1.6660e-02,\n                      -2.3457e-02,  4.6057e-03,  3.1110e-02, -3.6720e-03, -1.5362e-02,\n                      -9.1451e-03, -2.8676e-02, -2.3561e-02, -1.3353e-02, -1.9835e-02,\n                       8.7330e-03,  2.7899e-03,  3.3188e-02, -2.9035e-02, -3.4373e-02,\n                      -2.1642e-02, -3.4988e-02, -2.1040e-02, -5.1173e-03,  2.9482e-02,\n                      -1.1489e-02, -1.3534e-02,  2.4765e-02, -2.2795e-02, -3.0507e-02,\n                      -3.2282e-02,  1.9655e-02,  2.5654e-02, -1.5562e-02, -3.4733e-03,\n                       3.0941e-02, -3.5549e-02, -3.3390e-02, -7.5249e-03, -2.9531e-02,\n                      -2.4516e-02,  2.6714e-02,  3.2943e-04,  3.5501e-02, -2.0384e-02,\n                      -1.6564e-03,  1.3291e-02,  2.2487e-02, -5.4277e-05,  6.3617e-03,\n                       1.7872e-02, -7.6022e-03,  1.3880e-03,  6.9513e-03,  5.5381e-03,\n                      -2.1206e-02,  2.5653e-02, -1.7067e-02,  5.6472e-03,  1.4927e-02,\n                      -1.2071e-02, -3.2583e-02, -2.2964e-02, -1.0614e-03, -4.6606e-03,\n                      -3.4355e-02, -1.2608e-03,  3.3455e-02,  1.7708e-02, -3.6392e-03,\n                       2.0154e-02, -1.9663e-02, -2.7132e-02,  3.4190e-02,  3.5430e-03,\n                      -1.8092e-02,  5.2615e-03,  2.4088e-02, -3.8919e-03,  3.4682e-02,\n                      -1.9999e-03,  7.2423e-03,  1.2482e-02, -2.7231e-02,  1.4099e-02,\n                      -1.5364e-02,  9.1264e-04, -2.3665e-02, -2.0052e-02,  7.9559e-03,\n                      -1.3226e-02, -1.3582e-02,  4.0395e-04,  3.5608e-02,  1.8290e-02,\n                       1.3898e-03, -1.3963e-02, -3.0300e-03,  1.3806e-02,  2.0893e-02,\n                       2.7192e-02, -3.1589e-03, -6.9224e-03,  2.7170e-02, -2.1470e-02,\n                       1.2351e-02,  2.9966e-02, -1.1558e-02, -3.3041e-02, -2.7355e-02,\n                       2.6938e-02,  2.6537e-02, -2.5771e-02, -3.5286e-02,  3.0799e-02,\n                       1.8188e-02,  3.2164e-02,  1.9671e-02,  1.6261e-03,  6.9659e-04,\n                      -1.1979e-02,  1.1924e-02,  6.1350e-03, -2.1869e-02, -3.0860e-04,\n                      -1.2651e-02, -3.3500e-02, -3.0105e-02, -1.3797e-02, -2.5121e-02,\n                       1.2455e-02, -2.4652e-02, -1.5758e-02, -3.4674e-02, -9.0785e-03,\n                      -8.2701e-04,  1.4096e-02,  4.0681e-03,  3.0027e-02, -5.3308e-03,\n                      -1.8528e-03, -9.3864e-03,  8.3914e-03, -2.6184e-02,  2.4677e-02,\n                      -1.9853e-03,  3.2446e-02, -2.6467e-02,  4.7733e-03, -3.4535e-02,\n                       4.6190e-03,  3.5000e-02, -2.0830e-03,  2.2306e-02, -1.5815e-02,\n                      -2.5511e-02,  2.6937e-02,  3.0082e-02,  2.3003e-02,  3.2467e-02,\n                      -1.2793e-02, -3.3409e-02, -7.8400e-04,  1.1044e-02, -4.4295e-03,\n                      -3.9607e-03, -2.8444e-03,  1.8809e-02, -2.6671e-02, -1.9583e-02,\n                       1.9089e-02,  1.8431e-03, -1.8736e-02, -1.2343e-02, -3.5447e-02,\n                      -1.1781e-02,  1.9544e-02, -3.0213e-02, -4.8775e-03,  6.3440e-03,\n                       1.8245e-02, -2.9564e-02,  2.9970e-02, -2.2651e-02, -2.8822e-02,\n                       2.5229e-02, -1.4616e-02,  5.0890e-03, -2.7034e-02,  6.2822e-03,\n                      -1.4358e-02,  3.1759e-02, -1.0574e-02,  5.0225e-04, -1.2434e-02,\n                       1.0067e-02,  1.8638e-02, -1.5975e-02, -3.5000e-02,  2.9382e-02,\n                      -2.8447e-02, -1.3106e-02, -1.1737e-02, -3.3653e-02,  8.3820e-03,\n                       1.1344e-03,  2.0453e-02, -1.3324e-02,  2.3899e-02,  2.0907e-02,\n                      -2.8809e-02, -4.5196e-03, -3.5145e-02,  2.2744e-02,  3.3261e-02,\n                       9.9936e-03, -2.8413e-02,  7.8456e-03,  1.4902e-02, -2.3721e-02,\n                       3.5274e-02,  1.7168e-02, -1.8808e-02,  1.5260e-02, -1.8215e-02,\n                      -2.4084e-02,  9.9081e-03,  2.2768e-02,  2.4631e-02, -2.2718e-02,\n                       6.0747e-03,  1.6073e-03, -2.6383e-02, -3.0784e-02,  1.0728e-02,\n                       3.3754e-02,  2.4394e-02, -1.9524e-02,  2.7745e-02,  3.3266e-02,\n                       2.8523e-02, -1.3354e-02, -1.8376e-02,  1.0215e-02,  7.6692e-03,\n                       3.0158e-02,  1.5608e-02, -6.4061e-03, -1.7243e-02, -2.9242e-02,\n                       3.2167e-02,  1.0502e-02, -9.1221e-03,  2.1193e-02,  1.7250e-02,\n                       1.1439e-03, -3.0799e-02,  2.9887e-02, -1.2942e-02,  2.5792e-02,\n                      -1.8427e-02,  5.2473e-03, -2.5769e-02, -2.5449e-02,  9.4268e-03,\n                      -2.0851e-02,  1.6352e-02,  5.2191e-03, -6.5288e-03, -1.7271e-02,\n                      -2.1639e-02, -1.1359e-02,  2.9912e-02, -1.6127e-02,  2.8129e-02,\n                      -2.8871e-02,  3.1055e-02,  2.8573e-02,  2.2026e-02, -2.9571e-02,\n                      -3.7389e-03, -5.3312e-03, -3.3795e-02, -3.3367e-02,  2.6036e-02,\n                       2.4528e-02,  1.3716e-02, -5.3517e-03,  2.3745e-02,  3.2727e-02,\n                       3.9543e-03, -2.9076e-02, -3.1899e-02,  8.5699e-03,  1.5025e-03,\n                       1.7879e-02, -3.1036e-02,  1.9596e-02, -1.4988e-02,  1.5801e-02,\n                      -1.5764e-02,  1.7826e-02,  1.5960e-02,  1.7977e-02,  1.5977e-02,\n                      -2.0733e-02, -2.2928e-02, -3.3070e-02,  3.4617e-02,  7.7924e-03,\n                       2.2440e-02,  2.0348e-02,  1.7515e-02, -1.1775e-03, -3.8972e-03,\n                      -1.4392e-02, -3.5012e-02,  2.7830e-02, -3.0358e-02, -4.1687e-03,\n                       2.9801e-02,  9.2264e-03, -3.4764e-04, -2.1056e-02, -1.0588e-02,\n                       1.7082e-02,  2.3864e-02, -1.0106e-02,  2.3360e-02,  1.4005e-02,\n                      -1.2824e-02,  5.8648e-03, -1.8440e-02, -2.9656e-02, -1.5470e-02,\n                       1.3646e-05, -5.9589e-03,  1.7876e-03,  1.8521e-02, -3.5341e-02,\n                      -2.8084e-03, -2.7239e-02,  1.9851e-02,  2.9727e-02,  5.8796e-03,\n                      -1.1887e-02,  3.0943e-02,  2.8282e-02,  3.1754e-02, -6.6530e-04,\n                      -2.1568e-02, -2.5724e-02, -1.6968e-02, -1.6450e-03,  3.7423e-03,\n                      -2.4193e-02,  3.8047e-03, -5.5994e-03, -1.8458e-02,  6.7269e-03,\n                      -3.2713e-02,  3.1662e-02, -3.0478e-02, -2.1546e-02, -2.6901e-02,\n                       1.7879e-02, -3.5237e-02, -6.7105e-03,  7.5460e-03, -1.4208e-02,\n                      -8.2556e-05,  1.8086e-02,  2.7593e-03, -1.5701e-02, -3.0421e-02,\n                       1.1792e-02, -6.7388e-03, -3.2451e-02, -3.2661e-02, -2.9540e-02,\n                       2.9921e-03,  1.7188e-02, -6.7612e-03,  3.1605e-02, -2.7268e-02,\n                      -2.8262e-02, -1.9643e-02, -1.2356e-02,  8.9585e-03,  6.6730e-03,\n                      -9.1690e-03, -1.9600e-02, -3.2321e-02, -1.6104e-02, -5.1706e-03,\n                       3.1383e-02, -3.4907e-02,  1.7187e-02,  2.6123e-02, -1.5605e-02,\n                       2.8918e-02, -1.6128e-02,  2.4433e-02,  2.5184e-02, -1.3349e-02,\n                      -4.8808e-04,  2.4828e-02,  3.4424e-02, -1.0994e-02, -6.3396e-03,\n                      -2.6700e-02,  6.7134e-03, -3.0739e-02,  2.9507e-02, -1.9733e-02,\n                       1.2298e-02, -1.6927e-02,  2.7017e-02, -8.9675e-03,  3.5156e-02,\n                       3.4915e-02, -2.9157e-02, -2.8583e-02,  3.3909e-02,  3.6561e-03,\n                      -2.6225e-03, -1.8919e-02, -1.2331e-02,  2.4224e-02,  1.0636e-02,\n                       9.1910e-03,  2.0317e-02,  1.9835e-02, -3.3173e-02,  2.6771e-02,\n                      -2.1680e-03, -1.7251e-02, -1.7302e-02,  2.2881e-02,  3.4916e-02,\n                       5.5577e-03,  4.5560e-03,  2.2879e-02, -1.8620e-02,  2.4420e-02,\n                       3.3501e-02,  7.7311e-04,  8.5717e-03,  3.3039e-03, -6.3019e-03,\n                       1.5514e-02,  2.2109e-02,  1.7264e-02,  1.9235e-02,  3.2770e-02,\n                      -8.7934e-04, -1.5975e-03, -6.8949e-04,  1.3685e-02,  1.1100e-02,\n                      -5.0696e-03,  3.1239e-02,  6.9194e-03, -2.4311e-02, -3.3381e-02,\n                       1.6983e-02,  1.5671e-02,  2.4798e-02,  1.6588e-02, -3.4674e-02,\n                       2.5795e-02, -1.0557e-02,  1.5879e-02, -1.1953e-02,  8.9241e-03,\n                      -1.2576e-03,  1.5112e-02,  2.7895e-02,  1.3157e-02, -5.9604e-04,\n                      -1.2615e-02, -1.9451e-02, -1.1392e-02, -5.8876e-03, -3.3725e-02,\n                      -5.4041e-03,  2.9314e-03,  2.3190e-02,  3.0090e-02,  1.9957e-02,\n                       2.9270e-02, -2.3256e-03, -2.9112e-02,  4.0550e-04,  3.3252e-02,\n                       1.1473e-02,  2.8078e-03,  2.6532e-02,  2.8834e-02, -3.4229e-03,\n                      -2.7388e-02, -1.5284e-02, -2.9811e-02,  2.7250e-02, -1.0302e-03,\n                       5.3887e-03,  2.7224e-02, -2.0508e-02, -3.3193e-02,  5.4449e-03,\n                       2.8591e-02, -2.3438e-02,  2.6891e-02,  2.5819e-02, -2.4352e-02,\n                       2.3017e-02, -3.0108e-02, -3.3218e-02,  1.3881e-02,  2.8498e-02,\n                       8.8921e-03, -1.5504e-02, -3.0490e-02, -1.3592e-02, -4.8674e-03,\n                       3.6089e-03,  2.1107e-02,  3.1599e-02,  2.3500e-02,  3.1040e-02,\n                      -3.5547e-02, -1.5499e-02, -8.4581e-03,  3.4959e-02, -3.1111e-02,\n                       3.3227e-02, -7.7636e-03,  4.2232e-03,  2.3306e-02,  1.9567e-02,\n                       7.9290e-04,  2.0668e-02,  3.8562e-03,  3.2495e-02, -3.5149e-02,\n                       2.1403e-02,  3.0961e-02,  7.6941e-03,  1.8772e-05, -1.7525e-02,\n                      -3.5121e-02, -2.4088e-02,  8.6012e-04,  1.6683e-02, -2.5575e-02,\n                      -1.5966e-02,  1.4072e-02, -1.8487e-02,  2.4643e-03, -3.3133e-02,\n                      -7.3489e-04,  2.6096e-02, -2.5454e-02, -9.4229e-03, -6.8581e-03,\n                       1.5477e-02,  2.3825e-02, -3.4824e-02,  2.9749e-02,  4.6794e-03,\n                       1.5511e-02, -2.4102e-04, -1.4800e-02, -2.4109e-02, -1.7374e-02,\n                      -1.0352e-02, -2.0388e-02,  4.1069e-03, -2.2012e-02,  6.6894e-03,\n                      -4.9797e-03,  3.1117e-02, -1.3367e-02,  2.7166e-02,  9.9023e-03,\n                       2.4101e-02, -4.6616e-03, -2.4288e-02,  1.2726e-03,  1.4077e-02,\n                      -2.7659e-02,  1.9326e-02,  1.0348e-03, -2.1164e-02,  5.2113e-03,\n                      -2.4595e-02, -1.8745e-02,  3.6925e-03,  2.4325e-02, -1.7689e-02,\n                      -2.6724e-02, -1.0707e-02,  3.4162e-02, -2.6576e-02, -2.0972e-02,\n                       1.1342e-02, -3.0708e-02,  3.8346e-03, -8.7530e-03, -2.4769e-02,\n                      -1.3491e-02,  3.3170e-02, -1.0895e-02, -1.2031e-02, -5.3994e-03,\n                       3.2663e-02, -6.6928e-03, -3.1068e-02, -5.7127e-03,  9.3943e-03,\n                      -1.8417e-02, -2.7544e-03, -1.9244e-02,  1.9571e-02, -2.5623e-02,\n                       2.8005e-02, -2.1259e-02,  1.1110e-02, -1.9446e-02,  2.7398e-03,\n                       2.1628e-02,  2.8249e-02,  3.3246e-02, -3.3729e-02,  5.9243e-03,\n                       2.2534e-02, -3.5153e-02, -1.9447e-02,  2.2999e-02, -2.0265e-02,\n                      -2.4316e-02, -2.3335e-02,  1.4429e-03,  7.4817e-03,  1.1408e-02,\n                       3.9626e-03, -3.1588e-02, -4.8397e-03,  1.2116e-03, -1.1322e-02,\n                       6.9716e-03, -3.6034e-04,  2.2704e-02, -3.0948e-02,  2.8272e-02,\n                      -3.0361e-02, -7.7468e-03,  2.2014e-02,  9.5529e-03, -4.7003e-03,\n                      -2.5468e-02,  1.1673e-02,  1.9066e-02,  1.9745e-02,  1.1053e-02,\n                      -2.9781e-02,  2.9515e-02,  2.6103e-02,  3.8047e-03,  1.2079e-02,\n                       3.3243e-02, -8.9241e-04,  1.9420e-02,  1.4219e-02, -3.5050e-02,\n                       2.6877e-02, -3.1828e-02, -3.1807e-02,  3.5147e-02,  2.3303e-02,\n                       1.5597e-02, -2.1475e-02, -2.9660e-02, -2.6379e-02,  3.2444e-02,\n                      -2.1479e-02, -2.3162e-02,  3.0548e-02,  3.5063e-02,  1.5445e-02,\n                      -3.4288e-02, -1.5989e-02,  4.0240e-03,  3.2994e-02, -3.2733e-02,\n                       2.6296e-02, -1.9632e-02, -2.7331e-02, -1.6095e-02,  2.8140e-02,\n                      -6.8191e-03,  2.5377e-02,  2.7845e-02, -3.1201e-02,  1.5408e-02,\n                       2.0626e-02, -1.2456e-02, -8.2278e-03, -2.9899e-02, -5.1986e-03,\n                      -2.9695e-02, -1.1606e-02,  2.5533e-03,  2.6048e-02,  5.3130e-03,\n                      -2.4143e-02, -4.5323e-03,  1.7120e-02, -1.5021e-02,  5.6833e-03,\n                      -1.1634e-02, -7.3623e-03,  1.9340e-02, -2.3229e-03, -3.0511e-02,\n                      -2.7825e-02,  3.1052e-02,  3.4453e-02, -2.3666e-02,  1.0573e-02,\n                      -5.7773e-03, -2.0347e-02, -2.6792e-02,  2.4870e-02])),\n             ('fc2.weight',\n              tensor([[-0.0252,  0.0124, -0.0206,  ...,  0.0155, -0.0254, -0.0210],\n                      [-0.0297,  0.0079, -0.0073,  ..., -0.0168,  0.0028, -0.0251],\n                      [ 0.0278,  0.0121,  0.0176,  ...,  0.0153,  0.0107,  0.0254],\n                      ...,\n                      [-0.0067, -0.0059,  0.0082,  ...,  0.0239,  0.0071, -0.0066],\n                      [-0.0340,  0.0239,  0.0245,  ..., -0.0181, -0.0325, -0.0039],\n                      [-0.0084,  0.0334,  0.0243,  ...,  0.0355, -0.0117,  0.0310]])),\n             ('fc2.bias',\n              tensor([-1.2614e-02, -3.3590e-03, -3.0455e-02,  1.6434e-02, -7.6555e-03,\n                      -2.4152e-02, -5.5932e-03, -3.6875e-03, -2.8082e-02, -2.9735e-02,\n                      -3.2489e-02, -8.8659e-03, -1.0089e-03, -2.8699e-02,  1.3795e-02,\n                      -9.5827e-03,  3.1900e-02,  2.1102e-02,  3.4995e-02, -2.8433e-02,\n                      -2.4340e-02,  2.3105e-02, -2.6474e-02, -2.9296e-02, -1.3184e-02,\n                       3.2280e-02,  1.1905e-04,  2.3646e-02, -4.6278e-03, -2.2223e-03,\n                      -2.5446e-02,  3.2731e-03, -1.1366e-02, -4.4707e-03,  2.4884e-02,\n                      -1.5330e-02,  2.6318e-02,  7.6944e-03, -1.0975e-02,  2.8977e-02,\n                      -2.9667e-02, -3.1830e-02,  3.3165e-02, -1.4660e-02, -2.6671e-02,\n                       2.8189e-02, -2.5719e-02,  1.4545e-03, -2.9850e-02,  1.4369e-02,\n                      -3.5495e-02, -2.9094e-02,  3.3754e-02,  2.1157e-02,  3.6452e-03,\n                       1.5363e-02,  2.6282e-02, -2.2817e-02, -8.9026e-03, -1.3757e-02,\n                      -1.4916e-02,  1.1752e-02,  1.9164e-02, -1.9870e-02,  2.9574e-02,\n                       1.7716e-02,  4.5462e-03, -2.3659e-02,  1.7047e-02, -3.4305e-02,\n                       2.3785e-02,  7.5018e-04, -3.1228e-02,  3.2500e-02,  1.0074e-02,\n                      -1.0416e-02,  1.4756e-02, -3.0402e-02, -2.8649e-02, -1.2652e-02,\n                       1.5508e-03, -3.3269e-02,  2.1739e-02, -1.9829e-02, -2.6020e-02,\n                       1.8322e-02,  2.7139e-03, -2.0334e-04, -5.5006e-03, -2.0409e-02,\n                       1.7829e-02,  3.1028e-02,  4.0570e-03,  1.9469e-02, -3.5067e-03,\n                       5.1850e-03, -2.4040e-02,  1.8446e-02, -3.0092e-02,  3.0004e-02,\n                       3.3718e-02,  3.1496e-02, -2.9883e-02,  5.8339e-03, -2.1598e-03,\n                       8.1060e-03, -2.0465e-02,  8.3187e-03, -3.1497e-02,  2.7815e-02,\n                      -2.5940e-02, -1.4713e-04, -1.3706e-02,  3.4140e-02, -1.6163e-02,\n                      -1.5846e-02, -1.5787e-02,  3.0030e-02, -2.6285e-02, -1.4991e-02,\n                      -1.9451e-02,  2.9198e-02, -1.6015e-03, -1.9485e-03, -2.0570e-02,\n                       1.8667e-02,  2.7204e-02, -1.6337e-02, -6.3450e-03, -9.4404e-03,\n                      -2.5882e-02, -3.4440e-02, -3.1562e-02,  1.8097e-02, -2.3260e-02,\n                       3.5289e-02,  2.2858e-02,  4.3443e-03, -2.5291e-02,  3.0808e-02,\n                      -1.1564e-02, -1.0098e-02,  3.5506e-02,  5.0038e-03,  2.8942e-02,\n                      -3.1878e-02,  1.0642e-02,  1.9600e-02, -2.8410e-02, -2.1617e-02,\n                       1.4109e-02, -2.5884e-02, -3.2675e-02, -8.7181e-04,  5.7068e-03,\n                      -1.3889e-03,  3.1290e-02,  1.3377e-02,  8.4613e-03,  1.3738e-03,\n                      -1.7520e-02, -8.9993e-03,  1.0436e-02, -2.9194e-02, -1.7809e-02,\n                      -2.6666e-02,  3.4069e-02, -1.6217e-02,  2.6054e-02,  3.5310e-03,\n                       8.9352e-03,  6.7605e-03, -2.7099e-02, -2.8195e-02,  3.1747e-02,\n                      -6.4912e-03, -3.6162e-03, -2.5026e-02, -2.3036e-02, -1.8584e-02,\n                       2.8071e-02,  2.4944e-02,  1.3525e-02, -3.5316e-02,  3.1780e-02,\n                      -2.3661e-02, -2.8683e-02,  2.7840e-02,  1.6855e-02, -3.1792e-03,\n                      -5.1847e-03,  2.2281e-02, -2.1638e-02, -1.1333e-02,  2.5191e-02,\n                       3.5390e-02,  1.7299e-03, -2.5377e-02, -3.0020e-02, -5.6519e-03,\n                      -1.8127e-02, -2.2390e-03, -1.6242e-02, -3.2095e-02,  5.4077e-03,\n                       3.2309e-02, -1.0352e-02,  3.5510e-02,  1.0453e-02,  2.6349e-02,\n                       2.8365e-02,  1.3633e-02, -2.4721e-02, -2.4188e-02, -6.5581e-04,\n                      -3.5152e-02,  2.4904e-03,  3.1784e-02, -1.1706e-02,  1.2296e-02,\n                      -2.5695e-02,  9.2047e-03,  1.7541e-02, -5.9549e-03,  4.2556e-03,\n                       8.4527e-03, -1.6004e-02, -3.4062e-02,  1.4458e-02, -2.2804e-03,\n                       7.2126e-03, -7.3045e-03, -3.1386e-02, -4.9484e-03, -7.0464e-03,\n                      -8.9010e-03,  1.0931e-03, -8.9497e-03,  2.7202e-02,  3.5320e-02,\n                       2.9999e-02, -2.9444e-02, -2.4649e-02,  1.9934e-02,  2.0796e-02,\n                      -7.0663e-03,  1.5239e-02,  4.1727e-03,  3.9628e-03,  2.8027e-02,\n                       1.4924e-02,  1.7016e-03, -1.2351e-02, -1.5115e-03, -1.6765e-02,\n                      -2.6297e-02, -1.0269e-02,  2.9549e-02,  1.8449e-02,  2.3590e-02,\n                       2.1154e-02, -3.1399e-03, -8.6867e-03, -1.3350e-02, -2.2596e-02,\n                       2.1656e-02,  2.6292e-03, -2.3276e-02, -2.3270e-02, -2.5173e-02,\n                      -1.6106e-02, -3.1099e-03,  4.7168e-03, -2.9608e-02, -2.8112e-02,\n                      -2.3747e-02, -2.1599e-03, -2.2593e-02,  3.5372e-02, -7.6697e-03,\n                       2.2065e-02,  1.3011e-03, -2.1442e-02, -2.9035e-02,  1.8202e-02,\n                      -1.3052e-02, -2.4012e-02,  1.6914e-02,  2.5069e-02,  2.1523e-02,\n                       9.1590e-04,  3.4943e-02, -1.1088e-02, -4.8674e-03,  1.9195e-02,\n                       4.1406e-03, -1.4225e-02,  3.1152e-02,  3.0105e-02,  3.2277e-02,\n                       5.8408e-04,  2.3110e-02, -1.8989e-02,  7.3674e-03, -2.4394e-02,\n                      -3.3174e-02,  1.1254e-02, -2.2680e-02,  2.8256e-02,  1.7393e-02,\n                      -3.1773e-02,  2.3267e-02, -5.5444e-03,  2.8306e-02,  1.7785e-02,\n                       2.7797e-02, -1.2956e-02, -1.3169e-02,  1.6789e-02, -1.7215e-02,\n                      -1.3352e-02, -1.5772e-02, -1.7699e-02,  4.8014e-03,  6.4743e-04,\n                       1.6854e-02, -1.2380e-02,  1.1972e-03, -7.8176e-03,  1.7349e-02,\n                       2.3129e-02,  3.1423e-02, -1.8261e-02,  9.7769e-03,  2.5550e-02,\n                      -3.2273e-02,  2.9655e-02,  3.3530e-02, -3.7656e-03, -1.0217e-02,\n                      -1.6717e-02, -4.9634e-03,  1.0532e-02,  8.8457e-03,  1.1317e-02,\n                       1.9581e-02,  1.1888e-02, -2.9368e-02, -1.4681e-02,  3.1184e-02,\n                       2.5211e-02, -1.0732e-02,  3.5267e-02,  2.7438e-03,  3.5803e-03,\n                      -9.1685e-03,  3.3473e-02, -5.0501e-03,  3.1959e-02, -7.9562e-03,\n                       2.7016e-02,  8.0069e-03, -2.8071e-02,  1.5581e-02,  1.9647e-02,\n                      -2.3314e-02,  1.8724e-03, -2.7411e-02, -2.9864e-03, -6.6014e-03,\n                       7.3859e-03,  1.0479e-02,  3.5104e-02, -5.5646e-03,  1.3737e-02,\n                      -3.4102e-02,  1.6478e-02,  7.7129e-03, -2.3532e-02, -2.7448e-02,\n                      -1.4642e-02, -1.2635e-03,  4.2809e-03,  2.1337e-02,  8.9687e-04,\n                      -1.9659e-02,  3.3816e-02, -3.2582e-02, -7.1420e-03,  2.4228e-02,\n                       2.9770e-02,  8.9761e-03, -3.8964e-03,  7.1338e-03,  2.3671e-02,\n                      -9.2295e-03, -3.3618e-02,  2.3615e-02,  2.2162e-02, -1.1941e-03,\n                       1.5054e-03,  1.8462e-02,  1.7923e-02, -2.1171e-02,  2.5040e-02,\n                      -5.0449e-03, -2.8015e-02, -1.0137e-02, -3.0309e-02, -1.2158e-02,\n                       1.7468e-02, -2.6311e-02, -8.1421e-03, -3.4088e-02,  3.0113e-02,\n                       3.5933e-03,  5.0827e-03, -1.4633e-02,  2.7574e-02,  2.4496e-02,\n                      -3.2717e-02, -1.4532e-02,  1.6256e-02, -1.0562e-02, -2.9414e-02,\n                       2.1858e-02, -1.9510e-02,  1.2939e-02, -2.2618e-02,  1.6905e-02,\n                      -2.9554e-02,  2.1323e-02,  1.2142e-02,  3.3147e-02, -2.0048e-03,\n                       1.1414e-02, -3.4249e-02, -8.9830e-03, -3.4308e-02, -5.5570e-03,\n                      -3.2230e-02,  5.8580e-03,  2.9331e-02, -1.9140e-02,  4.3532e-03,\n                       1.3141e-02, -4.4404e-03, -1.1878e-02, -3.5646e-02, -1.7141e-02,\n                      -3.1754e-02, -5.6954e-03, -2.0984e-02, -2.6524e-02,  1.6314e-02,\n                       3.7264e-03, -1.3570e-02, -1.1447e-02,  3.0826e-02,  6.9444e-03,\n                       1.4661e-03,  1.5752e-02,  3.1203e-02,  2.7822e-02,  3.5041e-02,\n                      -1.6264e-02,  7.1961e-03,  8.3886e-03, -2.1087e-02, -6.1073e-03,\n                      -7.8093e-03,  2.3889e-02, -3.6619e-03,  3.2624e-02,  2.5871e-02,\n                       3.1241e-03,  1.1329e-02, -2.5993e-02, -1.2224e-02,  2.9960e-02,\n                      -2.8476e-02, -1.6884e-02, -1.2196e-02,  3.1647e-02, -3.8533e-03,\n                       1.9809e-02,  5.8581e-03, -2.6594e-02, -3.1839e-02,  3.4892e-03,\n                      -2.9874e-02, -2.1555e-02, -1.2776e-02,  7.1224e-03, -3.2801e-02,\n                       1.9169e-02, -1.8643e-02,  9.6916e-03, -1.8425e-03, -1.5047e-02,\n                       2.0999e-02, -3.5517e-02, -1.2181e-02,  2.7253e-02,  3.6861e-03,\n                       2.2454e-03, -3.0329e-02,  1.2797e-02, -2.9604e-02, -2.0709e-02,\n                      -9.4508e-03,  3.2569e-02,  9.3400e-03,  3.5152e-02, -3.3489e-02,\n                      -3.0612e-02, -9.3963e-03, -1.6871e-02, -2.8799e-02,  3.0303e-02,\n                      -8.1586e-03, -2.3199e-02,  1.3394e-02,  6.3510e-03,  9.4820e-04,\n                       1.1634e-02, -5.7317e-03,  3.3190e-02, -2.0557e-02,  1.8180e-02,\n                      -5.8671e-03, -2.3244e-02,  1.4557e-02,  7.1544e-03,  9.2550e-04,\n                      -3.7305e-03, -1.8871e-02, -2.7216e-02, -2.9927e-02,  3.2858e-02,\n                       4.0782e-03, -2.0451e-02,  3.4846e-02, -1.8719e-02,  1.5935e-02,\n                       1.3091e-02,  1.0566e-02, -2.4741e-02, -6.2631e-03,  2.5744e-02,\n                       6.4423e-04, -3.4771e-02, -2.8613e-02,  5.7839e-03,  6.9656e-04,\n                      -6.3336e-04,  2.4598e-02,  3.2688e-02, -1.8768e-02,  2.8848e-02,\n                      -6.1762e-03,  1.3958e-02,  3.0741e-02, -2.2623e-02, -1.5361e-02,\n                      -1.5824e-02,  1.7548e-02, -2.9560e-02, -1.4635e-02, -2.1013e-02,\n                       3.4164e-03,  7.6098e-03,  3.5683e-02,  2.9277e-02,  9.2102e-03,\n                      -4.4525e-04,  1.6026e-02, -2.3315e-02, -2.5326e-02,  6.0623e-04,\n                       2.1494e-02, -4.9552e-04,  5.5727e-03, -1.1832e-02, -1.1215e-02,\n                       1.3125e-02,  2.7894e-02,  1.7792e-02, -2.7848e-02,  2.3829e-02,\n                      -1.6914e-02,  2.0000e-02, -2.6433e-02,  1.4606e-02, -9.3848e-03,\n                       4.3160e-03,  3.8929e-03, -2.8144e-02, -2.9993e-02,  3.5107e-02,\n                      -3.2699e-03, -1.4713e-03, -2.4469e-03,  1.2060e-02,  3.7553e-03,\n                      -1.5022e-02, -2.1805e-02, -1.3785e-02,  2.1076e-02,  1.0756e-02,\n                      -7.9137e-03, -1.7330e-02,  9.7538e-03, -1.5841e-02,  4.6399e-03,\n                      -2.0007e-02, -1.5833e-02,  1.8193e-02,  2.5892e-02, -2.7276e-02,\n                      -2.0044e-04,  3.3037e-02, -3.4102e-03,  2.1928e-02, -5.7146e-03,\n                       3.5113e-02,  6.3902e-03, -2.1917e-02,  9.0259e-03, -1.2827e-02,\n                       1.3069e-02,  3.1757e-02, -1.0192e-02,  2.3189e-02, -1.4785e-02,\n                      -3.2076e-03,  1.1254e-02, -2.3458e-02, -2.4492e-02,  2.8086e-02,\n                      -1.0825e-02, -2.6114e-02, -8.4622e-03,  2.9824e-02,  2.6351e-02,\n                      -1.3187e-02,  8.1362e-03, -1.2782e-02,  4.2664e-03, -2.7892e-02,\n                       1.1040e-02, -2.0308e-02,  8.5217e-03, -3.3966e-02, -2.4681e-02,\n                       3.6798e-03, -2.1928e-02, -1.5645e-02,  2.9434e-02,  3.5673e-02,\n                       2.3865e-02,  7.9260e-03, -3.0991e-02, -3.2665e-02, -1.3117e-02,\n                       2.9736e-02,  2.6912e-04,  1.6152e-02,  3.5183e-02,  2.3029e-02,\n                      -1.1394e-02, -2.9606e-02,  2.7770e-02,  4.3008e-05, -2.1199e-02,\n                       1.2840e-02, -1.2238e-02,  3.0640e-02, -2.6306e-03, -2.1427e-02,\n                      -2.8886e-02,  1.9117e-03,  1.2471e-03, -3.1901e-03, -2.0210e-03,\n                       2.2075e-02,  3.3909e-02,  1.9034e-02, -3.1759e-02,  8.8590e-03,\n                       3.5149e-02, -3.3255e-02, -2.4703e-02,  2.3038e-02,  3.5244e-02,\n                      -2.8276e-02,  9.1653e-03, -2.0994e-02, -4.1170e-03,  2.3894e-02,\n                       3.0972e-02,  2.6293e-02,  1.6300e-02,  2.0273e-02, -3.3543e-03,\n                      -2.1938e-02,  2.7262e-02, -2.6138e-02, -3.4864e-03, -3.4690e-02,\n                       2.8231e-02,  2.2512e-02, -2.6908e-02,  4.6151e-03, -2.2795e-02,\n                       9.2986e-03, -2.7592e-02, -3.3846e-02,  2.6473e-02,  3.0618e-02,\n                       7.4557e-03, -2.0704e-02,  2.4280e-02, -1.6937e-03, -8.4930e-03,\n                       2.6573e-03,  9.4731e-04, -2.5642e-02,  2.9488e-02, -1.0332e-02,\n                      -3.5639e-02,  2.6310e-02,  1.0534e-02,  2.8296e-02, -1.0911e-02,\n                       2.7375e-02, -3.2804e-02, -2.4195e-02,  1.6715e-02, -7.5154e-03,\n                      -1.4061e-02,  2.2092e-02,  8.4311e-03, -1.5588e-02, -1.4549e-02,\n                      -2.5198e-02,  3.3687e-02,  9.5344e-03, -2.4730e-02, -2.1124e-02,\n                      -6.1808e-03, -2.7521e-02, -2.4351e-02, -2.1195e-02,  4.6988e-03,\n                      -8.1891e-03,  1.1283e-02, -2.3730e-02, -2.2832e-02,  2.4280e-02,\n                      -7.7872e-03,  7.8400e-03, -3.4029e-02,  9.1368e-03, -2.4302e-02,\n                       3.3186e-02, -3.0758e-02, -1.7813e-02,  2.2520e-02, -1.2428e-02,\n                      -1.7919e-03,  3.1948e-02, -2.2823e-02, -2.5679e-02,  3.4961e-02,\n                       1.1175e-02, -2.0068e-02, -1.1165e-02, -1.5328e-02, -1.1290e-02,\n                       2.2597e-02,  1.6686e-02,  6.5357e-03, -1.1121e-02])),\n             ('fc3.weight',\n              tensor([[ 0.0139, -0.0120,  0.0008,  ...,  0.0340,  0.0062, -0.0346],\n                      [-0.0225,  0.0199, -0.0209,  ..., -0.0351,  0.0266,  0.0092],\n                      [-0.0283,  0.0113, -0.0059,  ..., -0.0219,  0.0064,  0.0025],\n                      ...,\n                      [ 0.0249, -0.0337,  0.0019,  ...,  0.0237, -0.0143, -0.0093],\n                      [-0.0040,  0.0315,  0.0131,  ..., -0.0021, -0.0099,  0.0024],\n                      [ 0.0173, -0.0226, -0.0349,  ..., -0.0120, -0.0313,  0.0018]])),\n             ('fc3.bias',\n              tensor([-1.7665e-03, -1.3088e-02, -3.3836e-02,  2.3975e-02,  5.0221e-03,\n                      -5.0474e-03,  2.3950e-03,  1.4303e-02,  7.6919e-03, -3.0230e-02,\n                       1.5067e-02,  1.6759e-02, -2.0913e-02, -2.6810e-03, -5.4047e-03,\n                       2.1452e-02,  2.9209e-02,  2.0816e-02,  6.1424e-03, -3.9662e-03,\n                       6.4550e-03,  1.8751e-02, -2.6704e-02, -2.0054e-03, -3.4921e-02,\n                       2.5397e-02,  3.3607e-04, -1.6972e-02,  2.8925e-02, -9.8737e-03,\n                      -2.2634e-02,  2.7420e-04,  1.7623e-03,  1.3306e-02, -1.1515e-02,\n                      -2.1013e-02,  1.9676e-02,  3.4732e-02,  3.2426e-02,  5.7775e-03,\n                      -2.1579e-02, -2.6037e-02,  1.8736e-03,  9.9002e-03, -5.1202e-03,\n                      -7.6792e-03,  8.7897e-03,  2.2611e-02,  3.1560e-02,  3.5101e-02,\n                      -2.8541e-02, -2.7476e-02,  2.4358e-02, -2.1185e-02, -2.8234e-02,\n                      -2.1579e-02, -5.6882e-03, -3.1015e-02, -2.3713e-02,  1.1876e-02,\n                      -8.1790e-03, -1.3023e-02, -3.9323e-03,  3.0770e-02,  1.5697e-02,\n                       1.5672e-02,  2.2602e-02,  2.4040e-02, -3.0890e-02, -2.7639e-02,\n                       2.6304e-02,  2.5806e-02, -2.2105e-02,  4.0385e-03, -2.5837e-02,\n                       3.4788e-02, -1.8106e-02,  2.1159e-02, -1.5292e-02,  2.8558e-02,\n                       2.0345e-02, -1.8574e-02, -4.2184e-03, -2.9565e-02,  1.7604e-02,\n                      -3.0018e-02, -1.2256e-02, -2.7909e-02,  2.7972e-02,  2.4221e-02,\n                      -6.5448e-03, -1.6516e-02,  5.3685e-03, -8.1860e-03, -6.5170e-03,\n                      -3.1379e-02, -2.5413e-02, -2.5751e-02,  2.5385e-02, -1.0260e-02,\n                      -5.5638e-03,  3.1228e-02, -1.5311e-02, -1.9387e-02, -5.0924e-03,\n                       3.0329e-02,  2.7763e-03, -2.3407e-02, -7.9094e-03,  5.1207e-03,\n                      -8.7089e-03, -2.2984e-02,  4.3634e-03, -3.1725e-02,  2.5269e-02,\n                       2.3267e-02, -9.4150e-03, -3.3474e-03, -3.0250e-02, -9.2363e-03,\n                      -2.4327e-02, -2.4361e-02,  1.6417e-02,  3.3183e-02,  3.2213e-02,\n                      -2.3062e-02, -1.3546e-04, -2.1460e-02,  3.0042e-02,  9.2429e-03,\n                      -1.9519e-02, -2.9609e-02,  3.3771e-03, -2.6675e-02, -2.2946e-02,\n                       2.8098e-02, -3.4973e-02,  1.6620e-02, -2.8228e-02,  1.3794e-02,\n                       6.8152e-03,  6.5989e-03,  7.4376e-03,  1.2361e-02,  3.0930e-02,\n                       3.9377e-03,  2.5423e-02, -3.9461e-03,  2.7939e-02, -3.1802e-02,\n                       2.7040e-02,  2.4020e-02,  5.5256e-03, -2.8172e-02, -1.5503e-02,\n                       1.4066e-02,  2.9091e-02,  2.6346e-02, -1.4514e-02, -2.0322e-03,\n                      -2.1060e-02, -2.8799e-02,  3.0565e-02, -3.5353e-02,  2.9636e-02,\n                      -3.1147e-02, -1.7095e-02,  7.7095e-04, -3.5135e-02, -2.5870e-02,\n                      -8.5618e-03,  6.5192e-03, -1.4489e-02, -3.4653e-02, -2.0319e-02,\n                      -2.1221e-02,  4.8749e-03,  2.3746e-02,  1.6568e-02,  2.3261e-02,\n                      -1.8010e-03,  3.4809e-03, -2.3232e-02,  5.0052e-03, -3.3933e-02,\n                       9.7744e-03,  1.2241e-02, -2.9258e-02,  1.7334e-03, -7.5377e-03,\n                      -1.3996e-02,  9.4878e-03,  1.4879e-02,  2.4323e-02, -7.1087e-03,\n                      -3.5090e-02,  7.7295e-03,  2.7782e-02, -2.4441e-02, -1.1657e-02,\n                      -3.1093e-02, -1.5956e-02, -2.1329e-03,  3.1250e-03, -2.0928e-03,\n                       1.6619e-02, -6.2808e-03,  2.2284e-02,  2.0524e-02,  3.7175e-03,\n                       1.1574e-02,  1.6797e-02, -2.3070e-02, -1.2352e-02, -1.4457e-03,\n                      -1.4542e-02,  2.5453e-02,  2.5493e-02, -6.0301e-03,  2.1191e-02,\n                       1.0517e-02, -1.8792e-02,  2.9082e-02,  2.4657e-02,  3.3790e-02,\n                       2.9486e-02, -2.2659e-02, -2.6533e-03,  2.7143e-02, -2.8993e-02,\n                      -5.4076e-03, -2.4998e-02,  1.3265e-02, -1.3140e-02,  3.0537e-02,\n                      -1.0698e-02,  8.1874e-03, -1.2523e-02,  3.1705e-02,  8.7502e-03,\n                       2.9729e-02,  2.7857e-02, -1.7207e-02,  5.2000e-03, -1.1241e-02,\n                      -8.4059e-03, -5.9600e-03, -2.6007e-02, -8.8805e-03, -3.2252e-02,\n                      -2.1412e-02,  3.5470e-02,  3.0724e-02, -1.7747e-02,  1.3595e-02,\n                      -3.7181e-03,  2.1297e-03,  2.3934e-02,  1.5092e-02, -1.2384e-02,\n                       2.8949e-02, -2.8731e-02,  3.1425e-02,  3.5003e-02, -1.9450e-02,\n                       2.3367e-02, -1.8747e-02, -1.6268e-02,  3.4135e-03, -3.3975e-02,\n                       2.9764e-02,  1.5327e-02,  1.6654e-02,  1.0010e-02, -3.5286e-02,\n                      -2.2807e-02, -1.4275e-02,  2.4994e-02, -1.5875e-02, -8.3442e-03,\n                       8.0533e-03, -2.8861e-04, -9.3438e-03, -3.3267e-02,  1.2525e-02,\n                       1.4396e-02, -2.6045e-02, -6.4257e-03, -6.5982e-03, -2.9026e-02,\n                       1.8181e-02, -3.4321e-03,  3.2782e-02,  1.8401e-02,  3.5626e-02,\n                       2.4673e-02,  2.3019e-03, -1.4639e-02, -7.3938e-03,  1.0070e-02,\n                       2.6066e-02,  1.3978e-02, -1.1179e-02, -2.0180e-02, -3.2236e-02,\n                      -3.3352e-02,  2.5179e-02,  3.0922e-02,  1.0620e-02,  3.4138e-02,\n                       1.3921e-02, -3.3262e-02,  2.2636e-02,  2.0302e-02, -5.3770e-03,\n                      -2.7888e-02, -1.7101e-02,  1.4628e-02, -2.2004e-02, -3.5177e-02,\n                       3.1026e-02,  2.3131e-02, -2.9156e-02, -3.5497e-02, -3.2011e-02,\n                      -7.3978e-03,  1.6029e-02, -2.8582e-03,  2.0827e-02,  2.0442e-02,\n                      -1.1391e-02, -4.5184e-03, -2.7409e-02,  9.1255e-05,  1.8737e-02,\n                       1.0292e-02, -2.8532e-02,  2.7311e-02,  4.4232e-03, -1.0491e-02,\n                      -1.5905e-02, -3.0404e-02, -2.6486e-02, -2.8450e-02,  1.3008e-02,\n                       2.9779e-02,  3.3092e-03, -8.8151e-03, -1.1626e-02, -3.0676e-02,\n                      -2.7027e-02, -1.3968e-02,  3.4556e-03, -1.7099e-02, -1.8548e-02,\n                       4.6861e-03, -1.2078e-02,  1.7354e-02, -5.6954e-03, -2.9497e-03,\n                       3.4231e-02, -3.2449e-02, -3.8471e-04,  2.2935e-02,  7.9514e-03,\n                       6.8812e-03,  1.4588e-03,  2.3280e-02,  1.6058e-02, -1.9717e-02,\n                       1.0709e-02,  2.0112e-02, -1.5412e-02,  2.0666e-02, -1.0874e-02,\n                      -2.7278e-02, -2.2968e-03,  1.3278e-02,  1.5106e-02,  1.6899e-02,\n                       3.3893e-02,  8.2843e-05, -2.0549e-02, -2.1435e-02,  1.4868e-02,\n                       5.1536e-04,  2.8488e-02, -3.5280e-02,  3.5032e-02,  1.8867e-02,\n                       2.8023e-03, -3.1164e-02,  1.2712e-02, -2.3762e-02, -1.9290e-02,\n                      -2.8173e-02, -9.0372e-03,  5.4292e-03, -1.7013e-02, -1.7121e-02,\n                       2.8696e-02,  4.6106e-03,  2.4416e-03,  2.3302e-02, -8.5694e-03,\n                      -3.4781e-02,  2.3355e-02, -6.1623e-03, -1.0542e-02, -1.3700e-02,\n                      -1.4767e-02,  1.8100e-02, -2.0187e-02, -2.0987e-02,  2.3352e-02,\n                       1.5307e-02, -2.6682e-02, -2.8159e-03,  1.7780e-03, -2.7697e-02,\n                      -3.5003e-02, -1.6922e-02, -3.6632e-04,  2.6331e-03,  1.5648e-02,\n                      -1.0539e-02,  1.0829e-02, -8.9894e-03, -1.0545e-02,  6.3475e-03,\n                       1.4931e-02,  2.6686e-02,  8.9621e-03, -3.1308e-02,  4.6628e-03,\n                       1.3254e-02,  2.2276e-02,  3.8747e-03,  3.3604e-02,  1.8592e-02,\n                       2.1669e-02,  9.4652e-03, -3.4446e-02, -2.6379e-02,  1.2358e-02,\n                       2.4009e-02,  2.5282e-02, -8.6911e-03, -2.0553e-02, -2.6450e-02,\n                       1.8342e-02, -1.3017e-02,  3.4468e-02, -2.7755e-02, -1.4918e-03,\n                       2.3064e-02,  2.2599e-02,  1.8499e-03,  8.8168e-03,  9.0103e-03,\n                       2.1970e-04,  1.8265e-02,  2.9109e-02,  2.8635e-02,  7.8201e-03,\n                       1.0025e-02, -2.4035e-02, -1.9273e-02,  3.7861e-03, -1.1929e-02,\n                      -7.7212e-03,  1.9659e-02,  1.8679e-02,  3.0167e-02,  1.5513e-02,\n                      -2.8100e-02, -3.8235e-03, -2.7575e-02,  1.2028e-02,  1.6404e-02,\n                      -1.9249e-02,  2.9415e-02,  2.8111e-02,  1.9384e-02,  1.7835e-02,\n                       1.9647e-02,  2.2749e-02,  3.1936e-02,  1.4007e-02,  3.3364e-02,\n                       3.5660e-02,  3.2439e-02,  2.7580e-02, -2.5113e-02, -9.5128e-03,\n                       2.6401e-02,  2.0466e-02,  2.7811e-02,  2.7396e-02, -5.1610e-03,\n                      -1.6304e-02,  8.0961e-03,  7.9402e-04, -3.7365e-03,  1.2509e-02,\n                       3.2282e-04,  2.9231e-02, -6.9475e-03, -3.3738e-02, -2.8941e-02,\n                       2.7899e-02, -3.4239e-02, -2.5785e-02,  2.8878e-02,  6.0775e-04,\n                      -1.8819e-02, -2.1029e-03, -1.8995e-03,  2.8268e-02, -1.9869e-02,\n                       3.4039e-02,  1.5667e-02, -2.6478e-02,  5.8216e-04, -1.8239e-02,\n                       1.1177e-02, -2.1570e-02,  6.6649e-03, -1.3072e-02,  1.5086e-02,\n                      -1.7197e-02,  1.7112e-02,  1.4818e-02, -1.0250e-02,  2.3325e-02,\n                      -1.8722e-02, -4.7979e-03,  7.3803e-03, -3.7703e-03,  7.5640e-03,\n                      -1.3384e-02,  2.0820e-02, -5.8379e-03, -1.0112e-02, -2.1220e-02,\n                      -1.5248e-03,  3.2420e-03, -3.4617e-02, -1.7721e-02,  3.6927e-03,\n                       2.9468e-02,  2.7097e-02,  1.3696e-03, -2.7222e-02, -3.1845e-02,\n                       1.2428e-02, -3.4349e-02, -1.0294e-03,  6.7858e-03,  2.3661e-02,\n                       7.3297e-03,  1.9550e-02, -3.2981e-02,  2.7961e-02, -4.6364e-03,\n                       7.7957e-03,  2.0500e-02, -2.0774e-02,  2.8477e-02, -7.1762e-03,\n                      -2.7165e-03, -5.0659e-03, -3.3941e-02, -2.1417e-02, -1.0297e-02,\n                      -1.6070e-02,  1.8912e-03, -2.9439e-02,  2.9557e-02,  2.9628e-02,\n                       2.6360e-02,  3.5378e-02,  5.3518e-03, -3.4292e-02, -3.4794e-02,\n                      -2.8582e-02, -1.9767e-02, -2.2698e-02,  8.9017e-03,  2.5903e-02,\n                      -2.3820e-02, -2.4886e-02, -1.2340e-02, -3.0791e-02, -1.7881e-02,\n                       6.8662e-03,  3.2387e-02, -2.6566e-02, -8.0934e-03,  1.6080e-03,\n                       2.3131e-02,  9.6449e-03,  2.7328e-02,  3.4310e-03,  2.8222e-02,\n                      -1.9853e-02, -2.2834e-02, -1.9656e-02,  2.9938e-02,  9.2560e-03,\n                       5.1026e-03,  2.3980e-02, -2.5760e-04, -2.2312e-02,  1.0297e-02,\n                      -2.5496e-02, -1.4493e-02, -6.2285e-03, -8.4332e-03, -1.8056e-02,\n                      -3.1481e-02, -9.3203e-03, -2.9739e-02,  2.6216e-02, -5.4663e-03,\n                      -6.6228e-03,  1.0754e-02, -2.3343e-02, -1.6953e-02, -2.5500e-02,\n                      -2.8342e-02, -1.4052e-02, -2.1768e-02, -2.7122e-02, -2.8881e-02,\n                      -9.6995e-03, -2.0574e-03, -1.4536e-02, -3.5041e-03, -1.6917e-02,\n                       3.0322e-02, -2.5568e-02, -1.2251e-02, -6.1559e-04,  1.3943e-02,\n                       1.7513e-02,  9.5382e-03,  3.3841e-02, -1.4681e-02, -2.2450e-02,\n                      -2.3101e-02, -8.3817e-03, -3.4891e-02, -3.1861e-02, -3.4956e-02,\n                       1.9735e-02,  3.3754e-02, -3.1037e-02, -5.3317e-03, -2.9244e-02,\n                      -1.9467e-02, -2.0110e-02, -3.2590e-02, -7.9655e-03,  6.4431e-03,\n                       1.9799e-02, -1.4018e-02,  4.5740e-03,  7.7179e-03,  1.3129e-02,\n                       2.6590e-02,  2.2119e-02, -3.4392e-02, -9.4693e-03, -1.2572e-02,\n                       7.6133e-03,  2.2309e-02, -3.5790e-03, -2.9458e-02,  3.3711e-02,\n                       2.0343e-02, -1.6909e-02,  2.8676e-04, -4.8656e-03, -1.9260e-02,\n                      -3.3456e-03,  3.0055e-02, -1.1475e-02,  1.9849e-02,  5.7678e-03,\n                       6.9648e-03, -1.0175e-02, -3.3481e-02, -9.3939e-03,  4.6971e-03,\n                      -1.4077e-02,  1.5764e-03, -3.3256e-02,  1.1059e-02, -1.7300e-02,\n                       2.8417e-02,  1.9819e-03, -2.8837e-02,  1.8231e-02, -3.5139e-02,\n                       3.5120e-02, -3.0813e-02,  3.2278e-02,  9.6632e-03, -2.7988e-02,\n                       2.0119e-02, -2.0845e-02,  1.9015e-03,  1.1158e-02, -1.8561e-03,\n                       3.3360e-02,  7.7953e-03,  9.4369e-03, -1.9068e-02, -3.5134e-02,\n                      -6.9457e-03, -2.8105e-02, -8.2680e-03, -1.1906e-02, -1.2654e-02,\n                       2.6761e-02, -1.6443e-02, -2.0178e-02,  2.9537e-03, -2.0335e-02,\n                       5.9461e-03, -2.2628e-02,  3.8924e-03,  1.7941e-02, -1.2084e-02,\n                       2.4356e-03, -2.0204e-02,  1.5602e-02,  5.9390e-03,  1.0308e-02,\n                       2.9036e-02, -1.1370e-02,  5.9011e-04,  7.9338e-03, -7.7957e-03,\n                       2.4102e-02,  2.1556e-02, -9.4663e-03, -5.3208e-03, -1.7752e-02,\n                       3.1027e-02, -2.9442e-02, -9.1337e-03, -6.5863e-03,  3.7345e-04,\n                       2.0842e-02,  1.4930e-02,  1.4401e-02, -1.9130e-02,  1.5596e-02,\n                      -2.7496e-03,  2.8726e-02,  1.4403e-02,  1.5593e-03,  2.6890e-02,\n                      -1.6767e-02, -1.6479e-02, -2.7080e-02, -4.6376e-03,  1.0242e-02,\n                       2.4889e-03, -2.7268e-03, -7.4415e-03,  3.0706e-02, -2.2181e-02,\n                      -3.2287e-02,  5.9514e-03,  2.8179e-03, -6.3407e-03, -5.4576e-03,\n                      -3.3923e-02, -3.3462e-02,  2.6984e-02, -3.2221e-03])),\n             ('fc4.weight',\n              tensor([[ 0.0138,  0.0028, -0.0017,  ..., -0.0032,  0.0297, -0.0106],\n                      [ 0.0262, -0.0210, -0.0049,  ...,  0.0297, -0.0313,  0.0344],\n                      [ 0.0214, -0.0213, -0.0292,  ...,  0.0088, -0.0234,  0.0095],\n                      ...,\n                      [-0.0340, -0.0274,  0.0260,  ...,  0.0017, -0.0321,  0.0081],\n                      [-0.0160,  0.0110, -0.0204,  ...,  0.0101, -0.0202, -0.0195],\n                      [-0.0165,  0.0004,  0.0224,  ...,  0.0231, -0.0131,  0.0326]])),\n             ('fc4.bias',\n              tensor([ 0.0302,  0.0087,  0.0015,  0.0015,  0.0302, -0.0200,  0.0072, -0.0017,\n                      -0.0292,  0.0057]))])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_feat.state_dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fc1.weight': tensor([[ 0.0172, -0.0161, -0.0075,  ...,  0.0059, -0.0047,  0.0017],\n",
      "        [-0.0267, -0.0219, -0.0302,  ..., -0.0029,  0.0012, -0.0163],\n",
      "        [ 0.0275, -0.0093, -0.0333,  ..., -0.0138,  0.0055, -0.0328],\n",
      "        ...,\n",
      "        [-0.0015, -0.0159, -0.0231,  ..., -0.0109,  0.0195, -0.0093],\n",
      "        [ 0.0065,  0.0274, -0.0327,  ...,  0.0053,  0.0243,  0.0175],\n",
      "        [-0.0229, -0.0293,  0.0004,  ..., -0.0110,  0.0073, -0.0074]]), 'fc2.weight': tensor([[ 9.1917e-03,  1.0989e-02,  2.6633e-02,  ..., -9.0834e-03,\n",
      "          1.9386e-05, -3.3072e-02],\n",
      "        [-9.9623e-03,  3.4387e-03,  3.2542e-02,  ..., -6.5294e-03,\n",
      "         -1.2326e-02,  1.0187e-02],\n",
      "        [ 3.4065e-02,  1.8922e-02,  5.3480e-03,  ..., -4.2586e-03,\n",
      "         -7.5498e-03,  2.1564e-02],\n",
      "        ...,\n",
      "        [-1.3778e-02, -3.2532e-02,  3.2409e-02,  ...,  7.7200e-03,\n",
      "          3.2663e-02, -2.3619e-02],\n",
      "        [-1.5580e-02,  1.9131e-03,  4.5237e-03,  ...,  2.5431e-02,\n",
      "         -3.0175e-03, -3.1466e-02],\n",
      "        [-7.0225e-03, -2.1857e-02,  1.3299e-02,  ..., -1.8625e-02,\n",
      "          2.7430e-02, -2.5590e-02]]), 'fc3.weight': tensor([[ 0.0274, -0.0184, -0.0008,  ..., -0.0173,  0.0200, -0.0158],\n",
      "        [-0.0138,  0.0093, -0.0241,  ...,  0.0241, -0.0133,  0.0054],\n",
      "        [-0.0049,  0.0289,  0.0294,  ...,  0.0175, -0.0046, -0.0126],\n",
      "        ...,\n",
      "        [-0.0052,  0.0138,  0.0209,  ...,  0.0281, -0.0351,  0.0041],\n",
      "        [ 0.0050, -0.0262, -0.0076,  ...,  0.0204, -0.0240,  0.0207],\n",
      "        [-0.0053, -0.0228,  0.0336,  ...,  0.0117,  0.0210,  0.0112]]), 'fc4.weight': tensor([[-4.4023e-02,  9.7007e-03, -3.9088e-02,  ...,  3.8186e-02,\n",
      "          4.2784e-02, -1.4585e-02],\n",
      "        [ 2.2783e-02, -4.7674e-02,  1.7196e-02,  ..., -3.3560e-02,\n",
      "          1.2688e-02, -3.9785e-05],\n",
      "        [ 9.2490e-03,  2.6281e-02, -3.3378e-02,  ...,  5.0503e-02,\n",
      "          5.7599e-02, -1.4137e-02],\n",
      "        ...,\n",
      "        [-6.3797e-02,  4.6690e-02,  1.5826e-02,  ...,  3.0134e-03,\n",
      "          3.3114e-02, -3.7679e-02],\n",
      "        [ 1.2000e-02, -3.6221e-02, -4.3818e-02,  ..., -2.1616e-02,\n",
      "          5.0902e-02, -9.6566e-03],\n",
      "        [-6.5246e-02,  2.9662e-02, -3.6242e-02,  ...,  3.7431e-02,\n",
      "         -3.7024e-02, -5.2165e-03]])}\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('results/mnist_random')\n",
    "\n",
    "# Extract the weights for each layer from the state dict\n",
    "weights = {}\n",
    "for key, value in state_dict.items():\n",
    "    if 'weight' in key:\n",
    "        weights[key] = value\n",
    "\n",
    "# Print the extracted weights\n",
    "print(weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['fc1.weight', 'fc2.weight', 'fc3.weight', 'fc4.weight'])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([784, 784])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['fc2.weight'].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([784])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mlp_feat.parameters())[1].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(mlp_feat.parameters()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#list(mlp_feat.parameters())[0] = weights['fc1.weight']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "Data = Torch_Dataset('mnist')\n",
    "train_x, train_y, test_x, test_y = Data.get_data()\n",
    "# WL stores processes relative to the Weakening process\n",
    "Weak = Weakener(Data.num_classes)\n",
    "Weak.generate_M(model_class='pll')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "z, w = Weak.generate_weak(train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "Data.include_weak(Weak.w)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "trainloader, testloader = Data.get_dataloader()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "mlp = MLP(Data.num_features,[Data.num_features,Data.num_features,Data.num_features],Data.num_classes,dropout_p=0.3)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = OSLCELoss()\n",
    "\n",
    "#mlp,results = train_and_evaluate(mlp,trainloader,testloader,optimizer,loss_fn,num_epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models.mlp'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresults/mnist_random.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m----> 3\u001B[0m     models \u001B[38;5;241m=\u001B[39m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'models.mlp'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('results/mnist_random.pkl', 'rb') as file:\n",
    "    models = pickle.load(file)\n",
    "\n",
    "\n",
    "#mlp = model_data['model']\n",
    "#results = model_data['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel_data\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model_data' is not defined"
     ]
    }
   ],
   "source": [
    "model_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'input_size', 'hidden_sizes', and 'output_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mMLP\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() missing 3 required positional arguments: 'input_size', 'hidden_sizes', and 'output_size'"
     ]
    }
   ],
   "source": [
    "MLP()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
