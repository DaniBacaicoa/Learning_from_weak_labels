{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quick example of WL Classification\n",
    "## using Valen's warm up and our virtual label model\n",
    "This example tries to show the usage of the classes and methods\n",
    "proposed here to show classification with weakly labeled datasets.\n",
    "\n",
    "We will use the model proposed by Valen and their warm up prior to 20 epochs\n",
    "with our method (We will use the"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Importing general libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Importing specific classes and functions\n",
    "from utils.weakener import Weakener\n",
    "\n",
    "#Importing drawing tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing dataloaders\n",
    "from datasets.openml_datasets import OpenML_Dataset\n",
    "from datasets.torch_datasets import Torch_Dataset\n",
    "from models.model import MLP,mlp_valen\n",
    "from utils.trainig_testing import train_model,evaluate_model,train_and_evaluate\n",
    "from utils.losses import EMLoss, CELoss, PartialLoss,OSLCELoss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# DS stores the dataset and its related attributes\n",
    "DS = Torch_Dataset('mnist')\n",
    "train_x, train_y, test_x, test_y = DS.get_data()\n",
    "# WL stores pocesses relative to the Weakening process\n",
    "WL = Weakener(DS.num_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Generation of the mixing matrix according to the model\n",
    "#  we've chosen.\n",
    "WL.generate_M(model_class='pll')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# We generate the weak labels (this step is optional as WL.virtual_labels(train_y) can do it)\n",
    "z, w = WL.generate_weak(train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "z is the numerical encodig of the weak label while w is a one-hot\n",
    "representation of that variable. z also encodes the row of the M matrix\n",
    "for that given weak label.\n",
    "\n",
    "Let's consider the example of partial label learning for the iris dataset.\n",
    "labels are encoded as ```{0: '011', 1: '101', 2: '110', 3: '111'}```\n",
    "so having an isntance with `z=3` means `w=[1,1,1]`, i.e., the weak label contains every label."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# We generate the weak labels (this step is optional as WL.virtual_labels(train_y) can do it)\n",
    "WL.virtual_matrix()\n",
    "#And we generate the virtual labels\n",
    "WL.virtual_labels(train_y)\n",
    "\n",
    "\n",
    "#we make a new dataloader because valen takes the weak label instead of the\n",
    "# virtual label.\n",
    "indices = torch.arange(0,len(train_x))\n",
    "valen_train_data = TensorDataset(train_x, w ,train_y, indices)\n",
    "valen_train_loader = DataLoader(valen_train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "WL.virtual_labels(train_y)\n",
    "DS.include_weak(WL.v)\n",
    "# Once we have our dataset with all the labels we need we can create the dataloaders.\n",
    "trainloader, testloader = DS.get_dataloader()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "mlp = MLP(input_size=DS.num_features, hidden_sizes=[DS.num_features,DS.num_features,DS.num_features],\n",
    "          output_size=DS.num_classes, dropout_p=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.2057 - Accuracy: 0.7010\n",
      "Epoch 2/10 - Loss: 0.1800 - Accuracy: 0.9153\n",
      "Epoch 3/10 - Loss: 0.1334 - Accuracy: 0.9358\n",
      "Epoch 4/10 - Loss: 0.1153 - Accuracy: 0.9429\n",
      "Epoch 5/10 - Loss: 0.1015 - Accuracy: 0.9497\n",
      "Epoch 6/10 - Loss: 0.0928 - Accuracy: 0.9519\n",
      "Epoch 7/10 - Loss: 0.0952 - Accuracy: 0.9523\n",
      "Epoch 8/10 - Loss: 0.0778 - Accuracy: 0.9592\n",
      "Epoch 9/10 - Loss: 0.0904 - Accuracy: 0.9562\n",
      "Epoch 10/10 - Loss: 0.0771 - Accuracy: 0.9604\n"
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.SGD(mlp.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn_bk = OSLCELoss()\n",
    "\n",
    "_,_,mlp =  train_model(mlp,valen_train_loader, optimizer, loss_fn_bk,num_epochs=10, return_model=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 0, 4,  ..., 5, 6, 8])\n",
      "torch.Size([60000, 10])\n",
      "torch.Size([60000, 784])\n",
      "torch.Size([60000, 10])\n",
      "torch.Size([60000, 10])\n",
      "torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "#Lets try with the values predicted from that model\n",
    "_,train_y_new = torch.max(nn.functional.softmax(mlp(train_x),dim=1),dim=1)\n",
    "print(train_y_new)\n",
    "train_y_new = torch.eye(DS.num_classes)[train_y_new]\n",
    "print(train_y_new.shape)\n",
    "WL2 = Weakener(DS.num_classes)\n",
    "WL2.generate_M(model_class='pll',pll_p=0.5)\n",
    "\n",
    "z2, w2 = WL2.generate_weak(train_y_new)\n",
    "WL2.virtual_matrix()\n",
    "#And we generate the virtual labels\n",
    "WL2.virtual_labels(train_y_new)\n",
    "indices = torch.arange(0,len(train_x))\n",
    "print(train_x.shape)\n",
    "print(torch.tensor(WL2.v).shape)\n",
    "print(train_y_new.shape)\n",
    "print(indices.shape)\n",
    "\n",
    "new_train_data = TensorDataset(train_x, torch.tensor(WL2.v) ,train_y_new, indices)\n",
    "new_train_loader = DataLoader(new_train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "new_test_data = TensorDataset(test_x, test_y)\n",
    "new_test_loader = DataLoader(new_test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "mlp2 = MLP(input_size=DS.num_features, hidden_sizes=[DS.num_features,DS.num_features,DS.num_features],\n",
    "          output_size=DS.num_classes, dropout_p=0.5, bn=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: Train Loss: 3.1698, Train Acc: 0.1047, Test Acc: 0.1031\n",
      "Epoch 2/30: Train Loss: 3.1705, Train Acc: 0.1035, Test Acc: 0.1018\n",
      "Epoch 3/30: Train Loss: 3.1536, Train Acc: 0.1037, Test Acc: 0.1013\n",
      "Epoch 4/30: Train Loss: 3.1658, Train Acc: 0.1042, Test Acc: 0.1029\n",
      "Epoch 5/30: Train Loss: 3.1712, Train Acc: 0.1017, Test Acc: 0.1016\n",
      "Epoch 6/30: Train Loss: 3.1646, Train Acc: 0.1037, Test Acc: 0.1016\n",
      "Epoch 7/30: Train Loss: 3.1763, Train Acc: 0.1034, Test Acc: 0.1021\n",
      "Epoch 8/30: Train Loss: 3.1468, Train Acc: 0.1040, Test Acc: 0.0980\n",
      "Epoch 9/30: Train Loss: 3.1529, Train Acc: 0.1039, Test Acc: 0.1003\n",
      "Epoch 10/30: Train Loss: 3.1626, Train Acc: 0.1026, Test Acc: 0.1007\n",
      "Epoch 11/30: Train Loss: 3.1652, Train Acc: 0.1043, Test Acc: 0.0989\n",
      "Epoch 12/30: Train Loss: 3.1867, Train Acc: 0.1016, Test Acc: 0.1024\n",
      "Epoch 13/30: Train Loss: 3.1801, Train Acc: 0.1032, Test Acc: 0.0994\n",
      "Epoch 14/30: Train Loss: 3.1527, Train Acc: 0.1022, Test Acc: 0.1037\n",
      "Epoch 15/30: Train Loss: 3.1664, Train Acc: 0.1029, Test Acc: 0.1027\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m CELoss()\n\u001B[0;32m      5\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(mlp\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m mlp2, results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmlp2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_train_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_test_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Print the results\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTrain Loss:\u001B[39m\u001B[38;5;124m'\u001B[39m, results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\utils\\trainig_testing.py:92\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[1;34m(model, trainloader, testloader, optimizer, loss_fn, num_epochs)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, vl, targets, ind \u001B[38;5;129;01min\u001B[39;00m trainloader:\n\u001B[0;32m     91\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 92\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inspect\u001B[38;5;241m.\u001B[39mgetfullargspec(loss_fn\u001B[38;5;241m.\u001B[39mforward)\u001B[38;5;241m.\u001B[39margs)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m     94\u001B[0m         loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, vl, ind)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\models\\model.py:32\u001B[0m, in \u001B[0;36mMLP.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m# Iterate over the linear layers and apply them sequentially to the input\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 32\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn:\n\u001B[0;32m     34\u001B[0m             x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_norms[i](x)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Define the optimizer and loss function\n",
    "mlp2 = MLP(input_size=DS.num_features, hidden_sizes=[DS.num_features,DS.num_features,DS.num_features],\n",
    "          output_size=DS.num_classes, dropout_p=0.5, bn=True)\n",
    "loss_fn = CELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "mlp2, results = train_and_evaluate(mlp2, new_train_loader, new_test_loader,\n",
    "                                  optimizer, loss_fn, num_epochs=30)\n",
    "\n",
    "# Print the results\n",
    "print('Train Loss:', results['train_loss'][-1])\n",
    "print('Train Accuracy:', results['train_acc'][-1])\n",
    "print('Test Accuracy:', results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2)\n",
    "\n",
    "# For Sine Function\n",
    "axis[0].plot(results['train_loss'])\n",
    "axis[0].set_title(\"Train_loss\")\n",
    "\n",
    "# For Cosine Function\n",
    "axis[1].plot(results['train_acc'])\n",
    "axis[1].plot(results['test_acc'])\n",
    "axis[1].set_title(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#We need to include this Weak Labels into the dataset,\n",
    "# we need to include w to mantain the coherence. z will only be used\n",
    "# in the loss for the EM\n",
    "DS.include_weak(WL.v)\n",
    "# Once we have our dataset with all the labels we need we can create the dataloaders.\n",
    "trainloader, testloader = DS.get_dataloader()\n",
    "\n",
    "print(len(testloader.dataset))\n",
    "print(len(trainloader.dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a trainloader containing (X,v,y) and a testloader containig (X,y)\n",
    "so we can just establish our model and train it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of the MLP class\n",
    "mlp = MLP(input_size=DS.num_features, hidden_sizes=[20],\n",
    "          output_size=DS.num_classes, dropout_p=0.5)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = CELoss()\n",
    "\n",
    "train_losses,train_accs=train_model(mlp,trainloader,optimizer,\n",
    "                                    loss_fn,num_epochs=10)\n",
    "\n",
    "# Print the final training loss and accuracy\n",
    "print('Final Training Loss: {:.4f}'.format(train_losses[-1]))\n",
    "print('Final Training Accuracy: {:.4f}'.format(train_accs[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2)\n",
    "\n",
    "# For Sine Function\n",
    "axis[0].plot(train_losses)\n",
    "axis[0].set_title(\"Train_loss\")\n",
    "\n",
    "# For Cosine Function\n",
    "axis[1].plot(train_accs)\n",
    "axis[1].set_title(\"Train_accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of the MLP class\n",
    "mlp = MLP(input_size=DS.num_features, hidden_sizes=[20],\n",
    "          output_size=DS.num_classes, dropout_p=0.5)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = CELoss()\n",
    "\n",
    "# Train and evaluate the MLP on the training data, test data, and get the results\n",
    "mlp, results = train_and_evaluate(mlp, trainloader, testloader,\n",
    "                                  optimizer, loss_fn, num_epochs=10)\n",
    "\n",
    "# Print the results\n",
    "print('Train Loss:', results['train_loss'][-1])\n",
    "print('Train Accuracy:', results['train_acc'][-1])\n",
    "print('Test Accuracy:', results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2)\n",
    "\n",
    "# For Sine Function\n",
    "axis[0].plot(results['train_loss'])\n",
    "axis[0].set_title(\"Train_loss\")\n",
    "\n",
    "# For Cosine Function\n",
    "axis[1].plot(results['train_acc'])\n",
    "axis[1].plot(results['test_acc'])\n",
    "axis[1].set_title(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-2fb0f616",
   "language": "python",
   "display_name": "PyCharm (Learning_from_weak_labels)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}