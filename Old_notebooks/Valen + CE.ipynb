{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from datasets.openml_datasets import OpenML_Dataset\n",
    "from datasets.torch_datasets import Torch_Dataset\n",
    "from utils.weakener import Weakener\n",
    "from models.model import MLP\n",
    "from models.general_model import MLP\n",
    "from utils.losses import PartialLoss,CELoss\n",
    "from utils.trainig_testing import train_and_evaluate,warm_up,ES_train_and_evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this case we will create 4 data structures so they contain the 4 Weak structures that support the 4 reconstruction matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rec_names = ['_pinv', '_opt', '_conv', '_optconv']\n",
    "batch_size = 16\n",
    "dataset_objects = []\n",
    "weakener_objects = []\n",
    "\n",
    "# Create dataset objects\n",
    "for name in rec_names:\n",
    "    dataset_name = 'Data' + name\n",
    "    dataset = Torch_Dataset('mnist', batch_size=batch_size)\n",
    "    dataset_objects.append(dataset)\n",
    "    globals()[dataset_name] = dataset  # store dataset object in a variable with the corresponding name\n",
    "\n",
    "# Create weakener objects\n",
    "for name in rec_names:\n",
    "    dataset_name = 'Data' + name\n",
    "    weakener_name = 'Weak' + name\n",
    "    weakener = Weakener(globals()[dataset_name].num_classes)\n",
    "    weakener_objects.append(weakener)\n",
    "    globals()[weakener_name] = weakener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rec_params = {'_pinv':{'optimize':False,'convex':False},\n",
    "              '_opt':{'optimize':True,'convex':False},\n",
    "              '_conv':{'optimize':False,'convex':True},\n",
    "              '_optconv':{'optimize':True,'convex':True}}\n",
    "for i,name in enumerate(rec_names):\n",
    "    weakener_objects[i].generate_M('pll')\n",
    "    _,train_y,_,_ =  globals()['Data'+name].get_data()\n",
    "    #_,train_y,_,_ =  dataset_objects[i].get_data()\n",
    "    _,_ = globals()['Weak'+name].generate_weak(train_y)\n",
    "    #_,_ = weakener_objects[i].generate_weak(train_y)\n",
    "    globals()['Weak'+name].virtual_matrix(convex=rec_params[name]['convex'],\n",
    "                                       optimize=rec_params[name]['optimize'])\n",
    "    #weakener_objects[i].virtual_matrix(convex=rec_params[name]['convex'],\n",
    "    #                                   optimize=rec_params[name]['optimize'])\n",
    "    globals()['Weak'+name].virtual_labels()\n",
    "    #weakener_objects[i].virtual_labels()\n",
    "    globals()['Data'+name].include_weak(globals()['Weak'+name].v)\n",
    "    #dataset_objects[i].include_weak(weakener_objects[i].v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_allocation(net,weights):\n",
    "    for name, param in net.named_parameters():\n",
    "        w = name.split('.')\n",
    "        w_name = 'fc'+str(int(w[1])+1)+'.'+w[2]\n",
    "        param.data = weights[w_name]\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for_pinvreconstruction starts:\n",
      "\n",
      "Epoch 1/100: Train Loss: 1.5164, Train Acc: 0.9425, Test Acc: 0.9457\n",
      "Epoch 2/100: Train Loss: -2.8712, Train Acc: 0.9390, Test Acc: 0.9266\n",
      "Epoch 3/100: Train Loss: -7.4562, Train Acc: 0.9357, Test Acc: 0.9262\n",
      "Epoch 4/100: Train Loss: -14.2761, Train Acc: 0.9260, Test Acc: 0.9234\n",
      "Epoch 5/100: Train Loss: -23.0331, Train Acc: 0.9219, Test Acc: 0.9338\n",
      "Epoch 6/100: Train Loss: -35.4214, Train Acc: 0.9163, Test Acc: 0.9149\n",
      "Epoch 7/100: Train Loss: -49.8262, Train Acc: 0.9127, Test Acc: 0.9116\n",
      "Epoch 8/100: Train Loss: -67.7955, Train Acc: 0.9106, Test Acc: 0.9155\n",
      "Epoch 9/100: Train Loss: -90.0142, Train Acc: 0.9059, Test Acc: 0.9032\n",
      "Epoch 10/100: Train Loss: -117.4532, Train Acc: 0.9048, Test Acc: 0.8944\n",
      "Epoch 11/100: Train Loss: -151.4129, Train Acc: 0.9002, Test Acc: 0.9134\n",
      "Epoch 12/100: Train Loss: -190.9467, Train Acc: 0.8987, Test Acc: 0.9014\n",
      "Epoch 13/100: Train Loss: -240.7222, Train Acc: 0.8965, Test Acc: 0.9016\n",
      "Epoch 14/100: Train Loss: -300.4070, Train Acc: 0.8948, Test Acc: 0.8923\n",
      "Epoch 15/100: Train Loss: -369.8680, Train Acc: 0.8944, Test Acc: 0.8827\n",
      "Epoch 16/100: Train Loss: -457.1862, Train Acc: 0.8864, Test Acc: 0.8933\n",
      "Epoch 17/100: Train Loss: -551.6515, Train Acc: 0.8910, Test Acc: 0.8789\n",
      "Epoch 18/100: Train Loss: -668.2488, Train Acc: 0.8876, Test Acc: 0.8867\n",
      "Epoch 19/100: Train Loss: -802.5221, Train Acc: 0.8840, Test Acc: 0.8717\n",
      "Epoch 20/100: Train Loss: -959.2945, Train Acc: 0.8856, Test Acc: 0.8748\n",
      "Epoch 21/100: Train Loss: -1138.8815, Train Acc: 0.8778, Test Acc: 0.8953\n",
      "Epoch 22/100: Train Loss: -1341.3481, Train Acc: 0.8829, Test Acc: 0.8864\n",
      "Epoch 23/100: Train Loss: -1582.8159, Train Acc: 0.8807, Test Acc: 0.8926\n",
      "Epoch 24/100: Train Loss: -1858.2103, Train Acc: 0.8762, Test Acc: 0.8713\n",
      "Epoch 25/100: Train Loss: -2153.2154, Train Acc: 0.8776, Test Acc: 0.8814\n",
      "Epoch 26/100: Train Loss: -2500.8179, Train Acc: 0.8777, Test Acc: 0.8706\n",
      "Epoch 27/100: Train Loss: -2892.5584, Train Acc: 0.8715, Test Acc: 0.8774\n",
      "Epoch 28/100: Train Loss: -3347.9007, Train Acc: 0.8733, Test Acc: 0.8766\n",
      "Epoch 29/100: Train Loss: -3841.8949, Train Acc: 0.8737, Test Acc: 0.8495\n",
      "Epoch 30/100: Train Loss: -4395.6871, Train Acc: 0.8719, Test Acc: 0.8631\n",
      "Epoch 31/100: Train Loss: -5017.6451, Train Acc: 0.8706, Test Acc: 0.8707\n",
      "Epoch 32/100: Train Loss: -5724.8022, Train Acc: 0.8675, Test Acc: 0.8743\n",
      "Epoch 33/100: Train Loss: -6482.7209, Train Acc: 0.8678, Test Acc: 0.8572\n",
      "Epoch 34/100: Train Loss: -7350.9984, Train Acc: 0.8670, Test Acc: 0.8596\n",
      "Epoch 35/100: Train Loss: -8303.1791, Train Acc: 0.8623, Test Acc: 0.8709\n",
      "Epoch 36/100: Train Loss: -9354.6646, Train Acc: 0.8633, Test Acc: 0.8656\n",
      "Epoch 37/100: Train Loss: -10491.2501, Train Acc: 0.8623, Test Acc: 0.8659\n",
      "Epoch 38/100: Train Loss: -11810.9231, Train Acc: 0.8573, Test Acc: 0.8555\n",
      "Epoch 39/100: Train Loss: -13184.2104, Train Acc: 0.8603, Test Acc: 0.8645\n",
      "Epoch 40/100: Train Loss: -14681.7372, Train Acc: 0.8584, Test Acc: 0.8635\n",
      "Epoch 41/100: Train Loss: -16363.4362, Train Acc: 0.8572, Test Acc: 0.8588\n",
      "Epoch 42/100: Train Loss: -18214.8200, Train Acc: 0.8547, Test Acc: 0.8621\n",
      "Epoch 43/100: Train Loss: -20241.7822, Train Acc: 0.8539, Test Acc: 0.8614\n",
      "Epoch 44/100: Train Loss: -22377.4487, Train Acc: 0.8542, Test Acc: 0.8634\n",
      "Epoch 45/100: Train Loss: -24826.8598, Train Acc: 0.8491, Test Acc: 0.8482\n",
      "Epoch 46/100: Train Loss: -27464.7368, Train Acc: 0.8508, Test Acc: 0.8558\n",
      "Epoch 47/100: Train Loss: -30087.7854, Train Acc: 0.8536, Test Acc: 0.8559\n",
      "Epoch 48/100: Train Loss: -33211.7715, Train Acc: 0.8501, Test Acc: 0.8490\n",
      "Epoch 49/100: Train Loss: -36457.8161, Train Acc: 0.8460, Test Acc: 0.8405\n",
      "Epoch 50/100: Train Loss: -40055.7844, Train Acc: 0.8466, Test Acc: 0.8435\n",
      "Epoch 51/100: Train Loss: -43901.7402, Train Acc: 0.8476, Test Acc: 0.8479\n",
      "Epoch 52/100: Train Loss: -48163.1173, Train Acc: 0.8442, Test Acc: 0.8302\n",
      "Epoch 53/100: Train Loss: -52605.0510, Train Acc: 0.8439, Test Acc: 0.8518\n",
      "Epoch 54/100: Train Loss: -57165.5532, Train Acc: 0.8431, Test Acc: 0.8533\n",
      "Epoch 55/100: Train Loss: -62689.9459, Train Acc: 0.8391, Test Acc: 0.8389\n",
      "Epoch 56/100: Train Loss: -67895.8968, Train Acc: 0.8412, Test Acc: 0.8394\n",
      "Epoch 57/100: Train Loss: -74239.9661, Train Acc: 0.8390, Test Acc: 0.8285\n",
      "Epoch 58/100: Train Loss: -80371.1086, Train Acc: 0.8384, Test Acc: 0.8426\n",
      "Epoch 59/100: Train Loss: -87456.4662, Train Acc: 0.8362, Test Acc: 0.8100\n",
      "Epoch 60/100: Train Loss: -94245.7809, Train Acc: 0.8365, Test Acc: 0.8328\n",
      "Epoch 61/100: Train Loss: -101946.3687, Train Acc: 0.8346, Test Acc: 0.8417\n",
      "Epoch 62/100: Train Loss: -110312.4257, Train Acc: 0.8334, Test Acc: 0.8433\n",
      "Epoch 63/100: Train Loss: -119033.4910, Train Acc: 0.8330, Test Acc: 0.8343\n",
      "Epoch 64/100: Train Loss: -128473.3857, Train Acc: 0.8310, Test Acc: 0.8446\n",
      "Epoch 65/100: Train Loss: -138675.5196, Train Acc: 0.8308, Test Acc: 0.8156\n",
      "Epoch 66/100: Train Loss: -149302.8311, Train Acc: 0.8277, Test Acc: 0.8277\n",
      "Epoch 67/100: Train Loss: -160756.1878, Train Acc: 0.8259, Test Acc: 0.8222\n",
      "Epoch 68/100: Train Loss: -172322.0338, Train Acc: 0.8280, Test Acc: 0.8362\n",
      "Epoch 69/100: Train Loss: -185362.7486, Train Acc: 0.8269, Test Acc: 0.8202\n",
      "Epoch 70/100: Train Loss: -198745.6854, Train Acc: 0.8269, Test Acc: 0.8296\n",
      "Epoch 71/100: Train Loss: -213057.6542, Train Acc: 0.8208, Test Acc: 0.8152\n",
      "Epoch 72/100: Train Loss: -227917.8278, Train Acc: 0.8244, Test Acc: 0.8362\n",
      "Epoch 73/100: Train Loss: -244318.9736, Train Acc: 0.8217, Test Acc: 0.8280\n",
      "Epoch 74/100: Train Loss: -261087.2942, Train Acc: 0.8192, Test Acc: 0.8360\n",
      "Epoch 75/100: Train Loss: -278795.3100, Train Acc: 0.8178, Test Acc: 0.8239\n",
      "Epoch 76/100: Train Loss: -297390.3320, Train Acc: 0.8193, Test Acc: 0.8108\n",
      "Epoch 77/100: Train Loss: -317275.2429, Train Acc: 0.8190, Test Acc: 0.8086\n",
      "Epoch 78/100: Train Loss: -338666.8309, Train Acc: 0.8164, Test Acc: 0.8226\n",
      "Epoch 79/100: Train Loss: -360148.2007, Train Acc: 0.8154, Test Acc: 0.8184\n",
      "Epoch 80/100: Train Loss: -383267.8966, Train Acc: 0.8124, Test Acc: 0.8332\n",
      "Epoch 81/100: Train Loss: -407738.1845, Train Acc: 0.8152, Test Acc: 0.8267\n",
      "Epoch 82/100: Train Loss: -434243.9381, Train Acc: 0.8089, Test Acc: 0.8042\n",
      "Epoch 83/100: Train Loss: -459936.3965, Train Acc: 0.8102, Test Acc: 0.8240\n",
      "Epoch 84/100: Train Loss: -489488.4395, Train Acc: 0.8088, Test Acc: 0.8208\n"
     ]
    }
   ],
   "source": [
    "#We will do the trainig for all the models\n",
    "all_result = {}\n",
    "all_model = {}\n",
    "for i,name in enumerate(rec_names):\n",
    "    trainloader, testloader = globals()['Data'+name].get_dataloader()\n",
    "    mlp_feature = MLP(globals()['Data'+name].num_features,\n",
    "                      [globals()['Data'+name].num_features, globals()['Data'+name].num_features, globals()['Data'+name].num_features],\n",
    "                      globals()['Data'+name].num_classes, dropout_p = 0.0, bn = False, seed = 1,\n",
    "                  layer_init = lambda x: nn.init.kaiming_uniform_(x, a=math.sqrt(5))\n",
    "                      )\n",
    "    valen_weights = torch.load('results/mnist_random')\n",
    "    mlp_feature = weight_allocation(mlp_feature,valen_weights)\n",
    "    optim = torch.optim.Adam(mlp_feature.parameters(),lr=1e-5)\n",
    "    loss = CELoss()\n",
    "    print('\\nTraining for'+ name +'reconstruction starts:\\n')\n",
    "    mlp, results = ES_train_and_evaluate(mlp_feature,trainloader,testloader,\n",
    "                                      optimizer=optim,loss_fn=loss,num_epochs=100)\n",
    "    all_model[name] = {'model':mlp,'optimizer':optim,'loss':loss}\n",
    "    all_result[name] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "tableau_palette = mcolors.TABLEAU_COLORS\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "colors = list(tableau_palette.values())\n",
    "\n",
    "# Plot the first subplot (Train Loss)\n",
    "for i,name in enumerate(rec_names):\n",
    "    ax1.plot(all_result[name]['train_loss'],label='Train'+name,color=colors[i])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_title('Train Loss')\n",
    "\n",
    "\n",
    "# Plot the second subplot (Accuracies)\n",
    "for i,name in enumerate(rec_names):\n",
    "    ax2.plot(all_result[name]['train_acc'], label='Train Accuracy'+name,\n",
    "             color=colors[i])\n",
    "    ax2.plot(all_result[name]['test_acc'], label='Test Accuracy'+name,\n",
    "             color=colors[i],linestyle='--')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracies')\n",
    "#ax2.legend()\n",
    "legend_elements = [Line2D([0], [0], linestyle='-', color='black', label='Train Accuracy'),\n",
    "                   Line2D([0], [0], linestyle='--', color='black', label='Test Accuracy')]\n",
    "ax2.legend(handles=legend_elements)\n",
    "\n",
    "fig.suptitle('CE Learning Results', fontsize=18, fontweight='bold', y=1.05)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
