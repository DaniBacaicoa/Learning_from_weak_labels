{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quick example of WL Classification\n",
    "## using Valen's warm up and our virtual label model\n",
    "This example tries to show the usage of the classes and methods\n",
    "proposed here to show classification with weakly labeled datasets.\n",
    "\n",
    "We will use the model proposed by Valen and their warm up prior to 20 epochs\n",
    "with our method (We will use the"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Importing general libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Importing specific classes and functions\n",
    "from utils.weakener import Weakener\n",
    "\n",
    "#Importing drawing tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing dataloaders\n",
    "from datasets.openml_datasets import OpenML_Dataset\n",
    "from datasets.torch_datasets import Torch_Dataset\n",
    "from models.model import MLP,mlp_valen\n",
    "from utils.trainig_testing import train_model,evaluate_model,train_and_evaluate\n",
    "from utils.losses import EMLoss, CELoss\n",
    "\n",
    "from utils.warm_up import warm_up_benchmark"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# DS stores the dataset and its related attributes\n",
    "DS = Torch_Dataset('mnist')\n",
    "train_x, train_y, test_x, test_y = DS.get_data()\n",
    "# WL stores pocesses relative to the Weakening process\n",
    "WL = Weakener(DS.num_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Generation of the mixing matrix according to the model\n",
    "#  we've chosen.\n",
    "WL.generate_M(model_class='pll')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# We generate the weak labels (this step is optional as WL.virtual_labels(train_y) can do it)\n",
    "z, w = WL.generate_weak(train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "z is the numerical encodig of the weak label while w is a one-hot\n",
    "representation of that variable. z also encodes the row of the M matrix\n",
    "for that given weak label.\n",
    "\n",
    "Let's consider the example of partial label learning for the iris dataset.\n",
    "labels are encoded as ```{0: '011', 1: '101', 2: '110', 3: '111'}```\n",
    "so having an isntance with `z=3` means `w=[1,1,1]`, i.e., the weak label contains every label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# We generate the weak labels (this step is optional as WL.virtual_labels(train_y) can do it)\n",
    "WL.virtual_matrix()\n",
    "#And we generate the virtual labels\n",
    "WL.virtual_labels(train_y)\n",
    "\n",
    "\n",
    "#we make a new dataloader because valen takes the weak label instead of the\n",
    "# virtual label.\n",
    "indices = torch.arange(0,len(train_x))\n",
    "valen_train_data = TensorDataset(train_x, w ,train_y,indices)\n",
    "valen_train_loader = DataLoader(valen_train_data, batch_size=364, shuffle=True)\n",
    "\n",
    "\n",
    "WL.virtual_labels(train_y)\n",
    "DS.include_weak(WL.v)\n",
    "# Once we have our dataset with all the labels we need we can create the dataloaders.\n",
    "trainloader, testloader = DS.get_dataloader()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "mlp = mlp_valen(DS.num_features,DS.num_features,DS.num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin warm-up, warm up epoch 10\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "After warm up, test acc: 0.0000\n",
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "After warm up, test acc: 0.0003\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "After warm up, test acc: 0.0001\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "After warm up, test acc: 0.0001\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "After warm up, test acc: 0.0000\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "After warm up, test acc: 0.0003\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "After warm up, test acc: 0.0001\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "After warm up, test acc: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Z:\\Learning_from_weak_labels\\utils\\losses.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(output)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m mlp \u001B[38;5;241m=\u001B[39m \u001B[43mwarm_up_benchmark\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtestloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mZ:\\Learning_from_weak_labels\\utils\\warm_up.py:20\u001B[0m, in \u001B[0;36mwarm_up_benchmark\u001B[1;34m(model, train_loader, testloader)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBegin warm-up, warm up epoch \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m10\u001B[39m))\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m---> 20\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m features, targets, trues, indexes \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     22\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model(features)\n\u001B[0;32m     24\u001B[0m         L_ce, new_labels \u001B[38;5;241m=\u001B[39m partial_loss(outputs, partial_weight[indexes,:]\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mdetach(), \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:143\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    140\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:143\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    140\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:120\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 120\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    123\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:163\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    161\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mstorage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    162\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 163\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "mlp = warm_up_benchmark(mlp,trainloader,testloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "mlp_valen(\n  (fc1): Linear(in_features=784, out_features=784, bias=True)\n  (relu1): ReLU()\n  (fc2): Linear(in_features=784, out_features=784, bias=True)\n  (relu2): ReLU()\n  (fc3): Linear(in_features=784, out_features=784, bias=True)\n  (relu3): ReLU()\n  (fc4): Linear(in_features=784, out_features=10, bias=True)\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = CELoss()\n",
    "\n",
    "mlp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 2/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 3/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 4/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 5/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 6/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 7/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 8/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 9/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 10/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Train Loss: nan\n",
      "Train Accuracy: tensor(0.0987, dtype=torch.float64)\n",
      "Test Accuracy: tensor(0.0980, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the MLP on the training data, test data, and get the results\n",
    "mlp, results = train_and_evaluate(mlp, trainloader, testloader,\n",
    "                                  optimizer, loss_fn, num_epochs=10)\n",
    "\n",
    "# Print the results\n",
    "print('Train Loss:', results['train_loss'][-1])\n",
    "print('Train Accuracy:', results['train_acc'][-1])\n",
    "print('Test Accuracy:', results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2)\n",
    "\n",
    "# For Sine Function\n",
    "axis[0].plot(results['train_loss'])\n",
    "axis[0].set_title(\"Train_loss\")\n",
    "\n",
    "# For Cosine Function\n",
    "axis[1].plot(results['train_acc'])\n",
    "axis[1].plot(results['test_acc'])\n",
    "axis[1].set_title(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#We need to include this Weak Labels into the dataset,\n",
    "# we need to include w to mantain the coherence. z will only be used\n",
    "# in the loss for the EM\n",
    "DS.include_weak(WL.v)\n",
    "# Once we have our dataset with all the labels we need we can create the dataloaders.\n",
    "trainloader, testloader = DS.get_dataloader()\n",
    "\n",
    "print(len(testloader.dataset))\n",
    "print(len(trainloader.dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a trainloader containing (X,v,y) and a testloader containig (X,y)\n",
    "so we can just establish our model and train it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of the MLP class\n",
    "mlp = MLP(input_size=DS.num_features, hidden_sizes=[20],\n",
    "          output_size=DS.num_classes, dropout_p=0.5)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = CELoss()\n",
    "\n",
    "train_losses,train_accs=train_model(mlp,trainloader,optimizer,\n",
    "                                    loss_fn,num_epochs=10)\n",
    "\n",
    "# Print the final training loss and accuracy\n",
    "print('Final Training Loss: {:.4f}'.format(train_losses[-1]))\n",
    "print('Final Training Accuracy: {:.4f}'.format(train_accs[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2)\n",
    "\n",
    "# For Sine Function\n",
    "axis[0].plot(train_losses)\n",
    "axis[0].set_title(\"Train_loss\")\n",
    "\n",
    "# For Cosine Function\n",
    "axis[1].plot(train_accs)\n",
    "axis[1].set_title(\"Train_accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of the MLP class\n",
    "mlp = MLP(input_size=DS.num_features, hidden_sizes=[20],\n",
    "          output_size=DS.num_classes, dropout_p=0.5)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = CELoss()\n",
    "\n",
    "# Train and evaluate the MLP on the training data, test data, and get the results\n",
    "mlp, results = train_and_evaluate(mlp, trainloader, testloader,\n",
    "                                  optimizer, loss_fn, num_epochs=10)\n",
    "\n",
    "# Print the results\n",
    "print('Train Loss:', results['train_loss'][-1])\n",
    "print('Train Accuracy:', results['train_acc'][-1])\n",
    "print('Test Accuracy:', results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2)\n",
    "\n",
    "# For Sine Function\n",
    "axis[0].plot(results['train_loss'])\n",
    "axis[0].set_title(\"Train_loss\")\n",
    "\n",
    "# For Cosine Function\n",
    "axis[1].plot(results['train_acc'])\n",
    "axis[1].plot(results['test_acc'])\n",
    "axis[1].set_title(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-2fb0f616",
   "language": "python",
   "display_name": "PyCharm (Learning_from_weak_labels)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}