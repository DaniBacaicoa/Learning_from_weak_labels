{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import datasets.datasets as dtset\n",
    "import utils.losses as losses\n",
    "\n",
    "from utils.weakener import Weakener\n",
    "from models.model import MLP\n",
    "\n",
    "from utils.trainig_testing import train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"Experimental_results/Datasets.pkl\",\"rb\")\n",
    "Data,Weak = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danibacaicoa\\vscode_projects\\Learning_from_weak_labels\\.venv\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:336: FutureWarning: \n",
      "    Your problem is being solved with the ECOS solver by default. Starting in \n",
      "    CVXPY 1.5.0, Clarabel will be used as the default solver instead. To continue \n",
      "    using ECOS, specify the ECOS solver explicitly using the ``solver=cp.ECOS`` \n",
      "    argument to the ``problem.solve`` method.\n",
      "    \n",
      "  warnings.warn(ECOS_DEPRECATION_MSG, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "Weak.virtual_labels(p=None, optimize = False, convex = True) #This is to create Virtual Labels\n",
    "Data.include_weak(Weak.z)\n",
    "Data.include_virtual(Weak.v)\n",
    "trainloader,testloader = Data.get_dataloader(weak_labels='virtual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Loss: 1.9661, Train Acc: 0.7518, Test Acc: 0.8881\n",
      "Epoch 2/5: Train Loss: 1.8595, Train Acc: 0.7876, Test Acc: 0.9076\n",
      "Epoch 3/5: Train Loss: 1.7820, Train Acc: 0.8196, Test Acc: 0.9134\n",
      "Epoch 4/5: Train Loss: 1.7495, Train Acc: 0.8291, Test Acc: 0.9060\n",
      "Epoch 5/5: Train Loss: 1.7264, Train Acc: 0.8402, Test Acc: 0.8968\n",
      "Epoch 1/5: Train Loss: 1.9756, Train Acc: 0.7501, Test Acc: 0.8791\n",
      "Epoch 2/5: Train Loss: 1.8427, Train Acc: 0.7903, Test Acc: 0.9171\n",
      "Epoch 3/5: Train Loss: 1.7857, Train Acc: 0.8123, Test Acc: 0.9261\n",
      "Epoch 4/5: Train Loss: 1.7543, Train Acc: 0.8292, Test Acc: 0.9256\n",
      "Epoch 5/5: Train Loss: 1.7245, Train Acc: 0.8392, Test Acc: 0.9367\n"
     ]
    }
   ],
   "source": [
    "loss = losses.LBLoss()\n",
    "overall_results = {}\n",
    "overall_models = {}\n",
    "epochs = 5\n",
    "for i in range(2):\n",
    "    mlp = MLP(Data.num_features,[Data.num_features],Data.num_classes, dropout_p=0.5, bn=True, activation =  'gelu')\n",
    "    optim = torch.optim.Adam(mlp.parameters(),lr=1e-2)\n",
    "    mlp, results = train_and_evaluate(mlp,trainloader,testloader,optimizer=optim,loss_fn=loss,num_epochs=epochs,sound=1)\n",
    "    overall_results[i] = results\n",
    "    overall_models[i] = mlp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
