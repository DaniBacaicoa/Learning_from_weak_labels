{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example of the warm up\n",
    "We will try to understand Valen's Warm up and its associated loss Parial_Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from datasets.openml_datasets import OpenML_Dataset\n",
    "from datasets.torch_datasets import Torch_Dataset\n",
    "from utils.weakener import Weakener\n",
    "from models.model import MLP\n",
    "from utils.losses import PartialLoss\n",
    "from utils.trainig_testing import train_and_evaluate,warm_up\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "Data = Torch_Dataset('mnist',batch_size=8)\n",
    "train_x, train_y, test_x, test_y = Data.get_data()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "Weak = Weakener(Data.num_classes)\n",
    "Weak.generate_M(model_class='pll')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[0.        , 0.        , 0.        , ..., 0.        , 0.00217014,\n         0.00217014],\n        [0.        , 0.        , 0.        , ..., 0.00217014, 0.        ,\n         0.00217014],\n        [0.        , 0.        , 0.        , ..., 0.00217014, 0.00217014,\n         0.        ],\n        ...,\n        [0.00195312, 0.00195312, 0.00195312, ..., 0.00195312, 0.        ,\n         0.00195312],\n        [0.00195312, 0.00195312, 0.00195312, ..., 0.00195312, 0.00195312,\n         0.        ],\n        [0.00195312, 0.00195312, 0.00195312, ..., 0.00195312, 0.00195312,\n         0.00195312]]),\n array([[0, 0, 0, ..., 0, 1, 1],\n        [0, 0, 0, ..., 1, 0, 1],\n        [0, 0, 0, ..., 1, 1, 0],\n        ...,\n        [1, 1, 1, ..., 1, 0, 1],\n        [1, 1, 1, ..., 1, 1, 0],\n        [1, 1, 1, ..., 1, 1, 1]]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weak.M,Weak.Z"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "z,w = Weak.generate_weak(train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n         [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n         [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]),\n tensor([ 79, 937, 474, 303, 554], dtype=torch.int32),\n tensor([[0., 0., 0., 1., 0., 1., 0., 1., 1., 1.],\n         [1., 1., 1., 0., 1., 1., 0., 1., 0., 0.],\n         [0., 1., 1., 1., 1., 0., 0., 1., 0., 0.],\n         [0., 1., 0., 0., 1., 1., 1., 0., 0., 1.],\n         [1., 0., 0., 0., 1., 1., 0., 1., 0., 1.]], dtype=torch.float64))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:5,:],z[:5],w[:5,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([60000, 10]), torch.Size([60000]), torch.Size([60000, 10]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape,z.shape,w.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(Weak.c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danibacaicoa\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\cvxpy\\problems\\problem.py:1385: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0.00041072, 0.00045548, 0.00045243, ..., 0.00176967, 0.00176692,\n       0.00195311])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weak.generate_wl_priors()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "MLP(\n  (layers): ModuleList(\n    (0): Linear(in_features=784, out_features=784, bias=True)\n    (1): Linear(in_features=784, out_features=784, bias=True)\n    (2): Linear(in_features=784, out_features=784, bias=True)\n    (3): Linear(in_features=784, out_features=10, bias=True)\n  )\n  (batch_norms): ModuleList(\n    (0): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (dropout): Dropout(p=0.0, inplace=False)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(Data.num_features,[Data.num_features,Data.num_features,Data.num_features],Data.num_classes)\n",
    "mlp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -81.1979,  -29.1538,  -94.3030,  -94.1308,  -41.4572, -103.1898,\n",
      "            0.0000, -121.9277,  -69.8631,  -62.0304],\n",
      "        [ -95.9643,  -25.5036, -126.4390, -124.1810,  -46.9770,  -96.6644,\n",
      "            0.0000, -114.7425,  -80.5385,  -63.3069],\n",
      "        [ -81.3244,  -33.7703, -147.5850,  -34.4398,  -55.0380, -108.2492,\n",
      "            0.0000,  -56.5811,  -61.9946,  -42.3607],\n",
      "        [ -57.8899,  -20.8471, -107.9544,  -74.1862,  -38.1200,  -75.4376,\n",
      "            0.0000,  -88.0930,  -62.3543,  -74.1394],\n",
      "        [ -68.9095,  -41.5605, -111.0749,  -99.9285,  -82.2764,  -78.3565,\n",
      "            0.0000,  -56.1380,  -97.4619,  -95.8610]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      " tensor([[5.4476e-36, 2.1810e-13, 1.1084e-41, 1.3168e-41, 9.8938e-19, 1.4013e-45,\n",
      "         1.0000e+00, 0.0000e+00, 4.5585e-31, 1.1495e-27],\n",
      "        [2.1048e-42, 8.3933e-12, 0.0000e+00, 0.0000e+00, 3.9641e-21, 1.0454e-42,\n",
      "         1.0000e+00, 0.0000e+00, 1.0533e-35, 3.2075e-28],\n",
      "        [4.8003e-36, 2.1564e-15, 0.0000e+00, 1.1040e-15, 1.2511e-24, 0.0000e+00,\n",
      "         1.0000e+00, 2.6738e-25, 1.1915e-27, 4.0086e-19],\n",
      "        [7.2235e-26, 8.8348e-10, 0.0000e+00, 6.0444e-33, 2.7840e-17, 1.7293e-33,\n",
      "         1.0000e+00, 5.5166e-39, 8.3150e-28, 6.3337e-33],\n",
      "        [1.1830e-30, 8.9227e-19, 0.0000e+00, 4.0638e-44, 1.8528e-36, 9.3373e-35,\n",
      "         1.0000e+00, 4.1646e-25, 4.7084e-43, 2.3346e-42]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logsoftmax = torch.nn.LogSoftmax(dim=1)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "logit  = mlp(train_x)\n",
    "logp = logsoftmax(logit)\n",
    "p = softmax(logit)\n",
    "print(logp[:5],'\\n\\n',torch.exp(logp)[:5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.00217014 0.00217014]\n",
      " [0.         0.         0.         ... 0.00217014 0.         0.00217014]\n",
      " [0.         0.         0.         ... 0.00217014 0.00217014 0.        ]\n",
      " ...\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.         0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.        ]\n",
      " [0.00195312 0.00195312 0.00195312 ... 0.00195312 0.00195312 0.00195312]] \n",
      "\n",
      " tensor([ 79, 937, 474, 303, 554], dtype=torch.int32) \n",
      "\n",
      " [[0.         0.         0.         0.00195312 0.         0.00195312\n",
      "  0.         0.00195312 0.00195312 0.00195312]\n",
      " [0.00195312 0.00195312 0.00195312 0.         0.00195312 0.00195312\n",
      "  0.         0.00195312 0.         0.        ]\n",
      " [0.         0.00195312 0.00195312 0.00195312 0.00195312 0.\n",
      "  0.         0.00195312 0.         0.        ]\n",
      " [0.         0.00195312 0.         0.         0.00195312 0.00195312\n",
      "  0.00195312 0.         0.         0.00195312]\n",
      " [0.00195312 0.         0.         0.         0.00195312 0.00195312\n",
      "  0.         0.00195312 0.         0.00195312]]\n"
     ]
    }
   ],
   "source": [
    "print(Weak.M,'\\n\\n',Weak.z[:5],'\\n\\n',Weak.M[Weak.z][:5,:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#Im going to test the Partial loss\n",
    "target = Weak.w[:5,:]/torch.sum(Weak.w[:5,:],dim=1,keepdim=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "target[target>0]=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3168e-41, 0.0000e+00, 1.4013e-45,\n",
      "         0.0000e+00, 0.0000e+00, 4.5585e-31, 1.1495e-27],\n",
      "        [2.1048e-42, 8.3933e-12, 0.0000e+00, 0.0000e+00, 3.9641e-21, 1.0454e-42,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 2.1564e-15, 0.0000e+00, 1.1040e-15, 1.2511e-24, 0.0000e+00,\n",
      "         0.0000e+00, 2.6738e-25, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 8.8348e-10, 0.0000e+00, 0.0000e+00, 2.7840e-17, 1.7293e-33,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 6.3337e-33],\n",
      "        [1.1830e-30, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8528e-36, 9.3373e-35,\n",
      "         0.0000e+00, 4.1646e-25, 0.0000e+00, 2.3346e-42]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "rev = target*p[:5,:].clone().detach()\n",
    "print(rev)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1500e-27, 1.1500e-27, 1.1500e-27, 1.1500e-27, 1.1500e-27, 1.1500e-27,\n",
      "         1.1500e-27, 1.1500e-27, 1.1500e-27, 1.1500e-27],\n",
      "        [8.3933e-12, 8.3933e-12, 8.3933e-12, 8.3933e-12, 8.3933e-12, 8.3933e-12,\n",
      "         8.3933e-12, 8.3933e-12, 8.3933e-12, 8.3933e-12],\n",
      "        [3.2604e-15, 3.2604e-15, 3.2604e-15, 3.2604e-15, 3.2604e-15, 3.2604e-15,\n",
      "         3.2604e-15, 3.2604e-15, 3.2604e-15, 3.2604e-15],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "        [4.1646e-25, 4.1646e-25, 4.1646e-25, 4.1646e-25, 4.1646e-25, 4.1646e-25,\n",
      "         4.1646e-25, 4.1646e-25, 4.1646e-25, 4.1646e-25]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "s = rev.sum(dim=1).repeat(rev.size(1),1).transpose(0,1)\n",
    "print(s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1500e-27],\n",
      "        [8.3933e-12],\n",
      "        [3.2604e-15],\n",
      "        [1.0000e+00],\n",
      "        [4.1646e-25]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "ts = torch.sum(rev,dim=1,keepdim=True)\n",
    "print(ts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1450e-14, 0.0000e+00, 1.2185e-18,\n         0.0000e+00, 0.0000e+00, 3.9640e-04, 9.9960e-01],\n        [2.5077e-31, 1.0000e+00, 0.0000e+00, 0.0000e+00, 4.7229e-10, 1.2455e-31,\n         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 6.6138e-01, 0.0000e+00, 3.3862e-01, 3.8372e-10, 0.0000e+00,\n         0.0000e+00, 8.2008e-11, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 8.8348e-10, 0.0000e+00, 0.0000e+00, 2.7840e-17, 1.7293e-33,\n         1.0000e+00, 0.0000e+00, 0.0000e+00, 6.3337e-33],\n        [2.8407e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4489e-12, 2.2421e-10,\n         0.0000e+00, 1.0000e+00, 0.0000e+00, 5.6058e-18]], dtype=torch.float64)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev/ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1450e-14, 0.0000e+00, 1.2185e-18,\n         0.0000e+00, 0.0000e+00, 3.9640e-04, 9.9960e-01],\n        [2.5077e-31, 1.0000e+00, 0.0000e+00, 0.0000e+00, 4.7229e-10, 1.2455e-31,\n         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 6.6138e-01, 0.0000e+00, 3.3862e-01, 3.8372e-10, 0.0000e+00,\n         0.0000e+00, 8.2008e-11, 0.0000e+00, 0.0000e+00],\n        [0.0000e+00, 8.8348e-10, 0.0000e+00, 0.0000e+00, 2.7840e-17, 1.7293e-33,\n         1.0000e+00, 0.0000e+00, 0.0000e+00, 6.3337e-33],\n        [2.8407e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4489e-12, 2.2421e-10,\n         0.0000e+00, 1.0000e+00, 0.0000e+00, 5.6058e-18]], dtype=torch.float64)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev/s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "Data.include_weak(w)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "trainloader,testloader = Data.get_dataloader()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "o,t,y,ind = next(iter(trainloader))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) \n",
      "\n",
      " tensor([[1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
      "        [0., 1., 1., 0., 1., 0., 1., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 1., 1., 0., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 1., 1., 0., 1., 0., 1.],\n",
      "        [0., 1., 1., 0., 0., 0., 0., 1., 0., 1.]], dtype=torch.float64) \n",
      "\n",
      " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]) \n",
      "\n",
      " tensor([51717, 14374, 26521, 55507, 59210,  6137, 43415,  9774])\n"
     ]
    }
   ],
   "source": [
    "print(o, '\\n\\n', t, '\\n\\n', y,'\\n\\n', ind)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 1., 0., 0., 1., 1., 0., 1., 0., 0.],\n        [1., 1., 1., 1., 1., 0., 1., 0., 1., 0.],\n        [0., 1., 0., 0., 1., 0., 0., 1., 1., 1.],\n        [1., 0., 1., 1., 0., 1., 1., 0., 0., 1.],\n        [1., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 1.],\n        [1., 0., 1., 1., 0., 0., 0., 0., 0., 1.],\n        [0., 1., 0., 1., 1., 0., 1., 1., 0., 1.]], dtype=torch.float64)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset.tensors[1][[59922, 28394, 16042, 33821, 28968, 51396, 42273, 56490]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#optim = torch.optim.SGD(list(mlp.parameters()),lr = 1e-2, weight_decay = 1e-4, momentum = 0.9)\n",
    "#optim = torch.optim.Adam(mlp.parameters(),lr = 1e-2)\n",
    "\n",
    "#loss = PartialLoss(trainloader.dataset.tensors[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "#mlp,results = train_and_evaluate(mlp,trainloader,testloader,optim,loss,num_epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[ 0.0210,  0.0098, -0.0363,  ..., -0.0449, -0.0424,  0.0076],\n         [-0.0354,  0.0375, -0.0562,  ...,  0.0552, -0.0114, -0.0444],\n         [ 0.0339,  0.0040,  0.0408,  ..., -0.0582,  0.0558, -0.0314],\n         ...,\n         [ 0.0172, -0.0480, -0.0061,  ...,  0.0089, -0.0298, -0.0208],\n         [-0.0165, -0.0182, -0.0461,  ...,  0.0006, -0.0058,  0.0562],\n         [-0.0195,  0.0265, -0.0231,  ..., -0.0438, -0.0348,  0.0196]],\n        requires_grad=True),\n Parameter containing:\n tensor([ 2.4323e-02, -2.9067e-02, -4.6438e-03,  2.8989e-02, -2.2289e-02,\n         -4.3756e-03,  3.1826e-02, -3.0761e-02,  2.2375e-03,  1.9557e-02,\n          1.2819e-03, -1.4787e-02, -8.0491e-03,  1.7845e-02, -1.9567e-02,\n         -3.4515e-02, -2.5450e-02,  2.4920e-02, -3.5144e-02,  3.2672e-02,\n         -1.2957e-02,  2.3730e-02,  2.1464e-02,  7.9114e-03,  3.4305e-02,\n         -1.1410e-02, -2.9093e-02,  1.9956e-02,  2.6921e-02,  3.1814e-02,\n         -1.7220e-02,  6.5969e-03, -1.0410e-02,  2.1160e-02, -1.0899e-02,\n         -8.4229e-03,  1.6378e-03,  3.4575e-02, -1.0736e-02, -3.2481e-02,\n         -2.9783e-02, -1.5173e-02,  3.4919e-02,  2.8474e-02,  3.0018e-02,\n          2.8059e-02, -8.6624e-03,  3.1940e-02,  2.0500e-02,  2.0672e-02,\n          9.1817e-03,  3.8541e-03, -9.3578e-03, -3.2397e-02,  2.6355e-02,\n         -1.3114e-02, -2.0010e-02,  1.9142e-02, -3.3450e-02, -3.0739e-02,\n          1.5342e-02,  3.4611e-02, -3.1237e-02, -4.3144e-03, -1.4208e-02,\n         -3.5627e-02, -2.2483e-03, -1.9178e-03, -1.7277e-03,  2.4919e-02,\n          1.2508e-02, -2.0000e-02, -3.2631e-02, -2.8334e-02,  1.5702e-02,\n         -6.3335e-03, -2.3533e-02,  1.7668e-02,  2.8798e-02, -1.0225e-02,\n          2.2876e-02, -2.3259e-02,  3.1338e-02, -5.2740e-03, -1.8645e-02,\n         -1.2114e-02,  3.5193e-02, -2.8449e-02,  1.9969e-03,  2.3317e-02,\n         -1.3798e-02,  2.7168e-02,  1.0635e-02, -1.7765e-02,  2.0949e-02,\n         -1.0838e-02,  2.4949e-02,  2.4175e-02, -1.8421e-02, -2.6024e-02,\n         -2.3738e-02,  1.8495e-02, -2.6483e-03, -1.5135e-03,  1.7825e-03,\n         -2.7790e-02,  2.8766e-02,  8.3166e-03, -1.7502e-02, -1.7736e-02,\n          3.5184e-02,  2.1238e-02,  2.6873e-04,  5.5579e-03, -2.3617e-02,\n         -1.4782e-02,  2.5166e-02, -3.0904e-02, -1.0220e-02,  3.4871e-02,\n          1.8616e-02,  1.3749e-02, -1.6458e-02, -1.1533e-02,  3.5373e-02,\n         -2.5722e-03, -1.5782e-02, -2.7715e-02, -6.3429e-03, -3.2591e-02,\n         -1.6429e-02, -2.2993e-02, -2.8556e-02, -1.6961e-03, -2.5443e-02,\n          4.3056e-03,  3.0446e-02,  4.5817e-03, -3.1887e-02,  7.1361e-03,\n          1.9106e-02,  3.0161e-02,  5.5713e-03,  9.2982e-03,  1.9431e-02,\n         -7.9176e-03, -9.7280e-03, -3.3052e-02, -2.7056e-02, -1.6052e-02,\n         -2.2037e-02,  1.6686e-02, -3.2107e-02,  4.3053e-03,  1.9886e-02,\n         -1.4014e-02, -2.3217e-02,  2.7804e-02, -2.6227e-02,  2.1116e-02,\n          2.6324e-02,  4.9704e-03, -1.1536e-03, -2.5250e-02, -1.9589e-02,\n         -2.1476e-03,  1.0916e-02, -7.1114e-03,  3.3182e-02,  2.2953e-02,\n         -1.2557e-02,  8.0335e-03,  2.4492e-02,  3.6657e-03,  2.3350e-02,\n          3.3754e-02, -2.0166e-02,  1.0596e-02, -2.3222e-02,  3.2024e-02,\n          1.4991e-02, -2.5134e-02, -2.5912e-03,  3.1196e-02, -2.1343e-03,\n          6.3499e-03,  1.4626e-02, -6.5275e-03, -2.2419e-02,  1.1947e-02,\n         -7.6397e-03, -1.8178e-02, -2.2707e-02,  1.1251e-02,  1.4353e-02,\n         -2.9219e-03,  1.8508e-02, -3.0538e-02, -1.5122e-02, -1.3218e-02,\n         -8.2194e-03,  2.4778e-02,  3.2539e-02, -2.7447e-02, -1.8691e-02,\n          1.4858e-02,  5.7227e-03, -1.4351e-02, -2.1867e-02, -3.3505e-02,\n          1.0228e-02, -1.7140e-02,  1.5071e-02,  5.2078e-03,  2.0967e-02,\n         -9.5292e-03, -2.9411e-02,  2.4733e-02, -3.1087e-02, -2.3459e-02,\n         -2.3138e-02, -2.1039e-02, -3.1196e-02,  2.1078e-02,  1.7300e-02,\n          3.4384e-03, -2.6171e-02, -2.8330e-03,  3.4749e-02,  1.6156e-02,\n          1.3996e-02,  1.4620e-03, -2.5389e-02, -1.9366e-02, -1.2742e-03,\n          2.4126e-02, -2.7875e-02,  3.4154e-02,  1.9073e-02, -1.9549e-02,\n         -2.1796e-02,  1.3039e-02,  3.2282e-02,  1.5002e-02, -1.1286e-02,\n         -1.5736e-03,  1.4929e-02,  1.4926e-02, -7.9709e-03, -3.0701e-02,\n         -8.0099e-03,  2.4419e-02,  2.6730e-02, -3.0180e-02,  3.4077e-02,\n         -3.0243e-03,  3.1480e-02, -4.6173e-04,  4.9583e-03, -3.2747e-02,\n          4.1850e-03, -1.7965e-03,  6.4795e-03,  2.1996e-02, -3.2922e-02,\n         -2.4819e-03, -2.7712e-03,  1.5848e-02,  1.2601e-02, -1.2627e-02,\n          2.2387e-02, -2.2526e-02, -5.8543e-05, -1.5174e-02, -2.7811e-02,\n          8.0798e-03,  2.1610e-02,  2.4050e-02, -1.2377e-02, -9.8887e-03,\n         -7.5274e-04,  3.0804e-02, -3.1487e-02,  1.4161e-02,  1.3285e-02,\n         -3.1053e-02,  1.8462e-02,  3.2796e-02,  1.3607e-02,  2.1458e-02,\n          2.1472e-02, -1.9564e-02, -2.3155e-02,  2.4843e-02, -7.8928e-03,\n          5.4228e-03,  2.1507e-02, -3.2815e-03,  3.3638e-02,  1.4553e-02,\n         -2.2350e-02,  2.7252e-02, -1.9998e-02, -1.8965e-02, -2.5854e-02,\n         -2.9876e-02, -2.3434e-02, -3.1368e-02,  9.2897e-03,  5.6487e-03,\n         -1.3818e-02,  1.3536e-02,  1.9403e-02,  3.2949e-02, -2.5273e-02,\n          1.2621e-02,  5.0810e-04,  8.9978e-04,  4.2915e-03,  3.5696e-02,\n         -1.0411e-02,  9.6422e-04, -1.1650e-02, -2.0672e-02,  2.2200e-02,\n         -1.1283e-02,  2.2916e-02, -2.0544e-03, -1.1994e-02, -1.0649e-03,\n          2.5789e-02, -1.2715e-02, -1.8634e-02, -8.9832e-03, -3.3136e-02,\n          1.5095e-02, -2.6546e-02,  7.9238e-03, -3.4394e-02,  5.9405e-04,\n          1.2807e-02,  1.5498e-02,  2.9073e-02, -3.4620e-02,  2.7388e-02,\n          2.4038e-02, -2.5889e-03, -1.8794e-02,  1.6677e-02, -1.5074e-02,\n         -4.6031e-03, -5.6536e-03, -2.8704e-02, -1.9713e-02,  1.9872e-02,\n          2.7893e-02,  1.2994e-02, -1.3281e-02, -1.4294e-02,  2.7716e-02,\n         -5.1009e-03,  2.9127e-02, -3.3340e-03, -5.1064e-03, -2.5756e-02,\n          2.6863e-02, -2.4438e-02,  2.3821e-02,  7.1415e-03, -1.9642e-02,\n         -3.3446e-02,  3.1797e-02, -4.6447e-03, -7.5789e-03, -7.1308e-03,\n         -2.5426e-02,  2.5699e-02, -2.1384e-02, -2.0811e-03,  3.1038e-02,\n          2.8557e-02,  1.9189e-02,  2.1231e-02, -2.7718e-02, -2.4224e-02,\n          3.4861e-02, -1.1826e-02,  2.1374e-03,  1.6461e-02,  1.1945e-02,\n         -3.1968e-02,  2.3039e-03, -8.9909e-04, -1.8054e-02, -1.0301e-02,\n         -1.8757e-02, -5.4647e-03, -7.5474e-03,  1.9250e-02, -2.2824e-02,\n         -1.9351e-02, -2.1873e-02,  2.7166e-02, -2.6321e-02, -3.2436e-02,\n         -1.4563e-02,  2.5941e-02, -2.4972e-02, -1.9362e-02,  2.6587e-03,\n          1.5499e-02, -3.0841e-02, -2.4175e-02,  2.7453e-02,  1.3767e-02,\n          2.4751e-02,  2.7127e-02, -5.9103e-03,  2.6237e-02, -2.8306e-02,\n         -2.8054e-02,  3.1936e-02, -7.7914e-03, -4.1745e-04,  2.4453e-02,\n         -2.1119e-02, -3.3460e-02,  2.1658e-02,  8.1804e-03,  2.1876e-02,\n         -2.6978e-02, -2.3479e-02,  2.5793e-02,  9.4179e-03,  2.1669e-02,\n         -3.1098e-02, -2.1249e-02, -1.8756e-02, -1.1939e-02, -3.2483e-02,\n         -8.3356e-03,  2.7932e-02,  9.1203e-03, -1.0621e-02,  3.2560e-02,\n         -1.1345e-02, -1.1954e-02, -7.8045e-03,  3.9348e-03,  2.9141e-02,\n         -2.3201e-02,  1.4646e-02,  9.6233e-03, -2.6561e-02, -2.1752e-02,\n          1.8240e-02,  1.8960e-02,  4.0911e-03, -1.6018e-02, -3.5294e-02,\n          2.6649e-04,  2.1576e-02, -3.4229e-02, -3.4017e-02,  1.8128e-02,\n          2.5650e-02, -1.2206e-02,  1.4782e-02,  9.2482e-03,  3.0206e-02,\n          3.0683e-02,  9.0057e-03,  4.4895e-03, -1.8193e-02,  1.6270e-02,\n         -1.5615e-02,  2.3184e-02, -9.6827e-03, -3.4582e-02, -8.7582e-03,\n          2.1314e-02, -1.4710e-02, -2.4787e-02, -1.7345e-03, -2.6576e-02,\n         -1.0157e-02, -5.8071e-03,  2.1118e-02,  9.0575e-03,  2.9429e-02,\n         -2.2512e-02, -5.8716e-04,  3.2416e-02,  1.3776e-02,  2.8065e-02,\n          2.4401e-02,  6.2146e-03,  1.5144e-02, -7.1663e-04, -7.3998e-03,\n          3.4842e-03,  1.7395e-02,  3.0004e-02,  1.4540e-02, -2.1887e-02,\n          1.2051e-02, -1.7429e-02, -1.8135e-02,  2.3627e-03,  1.3572e-02,\n         -9.5186e-03,  9.9515e-03,  1.0988e-02,  1.8093e-02, -9.1311e-03,\n         -1.7913e-02,  3.2550e-02,  1.5670e-02,  2.2885e-02, -1.4204e-02,\n         -1.6866e-02,  1.6570e-02, -1.3319e-02,  3.5533e-02,  2.4512e-02,\n         -1.4900e-02,  1.0090e-02, -3.3991e-02, -2.6840e-02,  3.0151e-02,\n         -8.2534e-03, -1.5289e-02, -2.8235e-02,  1.2481e-02,  1.9816e-02,\n          3.2176e-02, -7.2834e-04,  4.6294e-03,  1.6766e-02,  2.1330e-02,\n         -9.5880e-03,  2.2633e-02, -3.3709e-02,  8.9316e-03, -9.8559e-03,\n         -2.6301e-02,  1.7008e-02,  5.3647e-03,  1.6243e-02, -1.7441e-02,\n         -3.4652e-02, -1.6659e-02, -1.9016e-02, -1.0941e-04, -3.3069e-02,\n         -2.0327e-02, -2.8458e-02, -1.6239e-02,  1.1476e-02, -9.2117e-03,\n         -1.2723e-03,  2.2649e-02,  6.1830e-03,  6.3898e-03, -1.7152e-02,\n         -2.8934e-02,  1.8479e-03,  3.2249e-02, -3.2809e-02, -1.0811e-02,\n         -3.5667e-02, -1.4237e-02,  1.2731e-02,  3.3918e-02, -2.4069e-02,\n         -8.9425e-03, -2.8332e-02, -2.8565e-02,  5.6284e-03, -2.0919e-02,\n         -1.2667e-02, -1.5034e-02, -1.1797e-02,  3.0032e-02, -3.3816e-02,\n         -3.0462e-02, -9.2117e-03,  8.4714e-03, -2.6466e-02, -1.5582e-02,\n         -2.3808e-02,  3.0395e-02,  2.4848e-02, -5.9737e-03, -3.1354e-03,\n          1.3387e-02, -1.5131e-02, -1.4246e-02, -1.3730e-02,  7.5548e-03,\n         -1.1775e-02, -1.1775e-02,  2.8975e-02,  2.8165e-02,  9.9257e-03,\n          2.1372e-02,  1.1901e-02, -3.1575e-02, -3.0937e-02,  1.7064e-02,\n         -4.1910e-04, -3.4711e-02, -2.4752e-02, -3.4965e-02, -2.5419e-03,\n          3.4302e-02, -3.1229e-02,  1.8586e-02,  1.5375e-02, -1.6348e-02,\n         -4.0114e-03, -1.2014e-02,  3.2525e-02,  5.9281e-03,  2.4760e-02,\n         -4.8698e-03,  3.8956e-04, -1.8537e-02, -2.8909e-02,  2.0854e-02,\n          3.3773e-02,  1.7686e-02, -1.3689e-02,  3.3845e-02, -3.0976e-02,\n         -1.1716e-02, -2.5583e-03,  1.8628e-02,  3.4720e-02, -1.8570e-02,\n         -2.3024e-02,  1.8815e-02, -1.6040e-02, -2.5533e-02, -2.2572e-02,\n         -1.2412e-02, -3.1219e-02, -4.4236e-03, -1.3312e-02,  5.7776e-03,\n          2.7909e-02, -5.2459e-03, -1.7216e-02, -1.4498e-02,  2.5304e-02,\n         -1.9894e-03,  1.0167e-02, -1.4542e-02,  1.4012e-02,  3.5505e-02,\n         -8.0304e-03, -1.3549e-02,  1.2719e-02, -2.7758e-02,  2.1478e-03,\n          1.7828e-02,  2.4534e-02,  7.6694e-04, -8.6758e-03,  3.3044e-02,\n          2.6378e-02, -1.1968e-03, -3.4031e-02,  2.1752e-02, -1.2179e-02,\n          6.9242e-03,  1.1170e-02,  3.5169e-02, -8.4722e-03, -2.1960e-02,\n          1.0984e-02,  1.0049e-04, -3.4215e-02,  2.7312e-02,  8.9315e-04,\n          8.0410e-03,  2.4663e-02, -2.3531e-02, -1.9032e-02,  3.1589e-02,\n         -1.9457e-02,  2.0939e-02,  1.2158e-02, -3.2002e-02,  3.1525e-02,\n         -1.7713e-02, -1.8154e-02,  1.1005e-02, -3.3463e-02,  1.8078e-02,\n          3.3078e-02,  6.0572e-03, -1.3034e-02,  1.9628e-02, -3.2842e-02,\n         -4.9725e-03,  2.7084e-03, -2.2557e-02, -1.8987e-02, -2.0292e-02,\n          3.5490e-02,  1.3126e-02, -2.3209e-02, -2.8207e-02, -4.2362e-03,\n          3.7608e-03,  2.4986e-02, -2.9033e-02, -7.4423e-03, -2.3510e-02,\n         -2.9334e-02,  1.6442e-02,  2.9568e-02,  3.4065e-02,  1.2421e-03,\n         -1.3247e-02,  6.0807e-03,  1.6265e-02, -8.1063e-03,  4.1246e-03,\n          7.9718e-03, -8.0876e-03, -1.3099e-02, -3.2714e-02, -1.4147e-02,\n         -4.9356e-03, -3.3415e-02,  8.1694e-03,  4.0933e-03,  1.0856e-02,\n         -1.7564e-02,  3.3194e-02, -2.1143e-02,  7.0756e-03, -2.2081e-02,\n          1.3415e-02,  1.1158e-02,  2.3184e-02,  1.0423e-02, -6.9765e-04,\n         -6.1029e-03, -2.0815e-02, -1.8653e-02, -2.1957e-02,  7.9923e-03,\n         -3.1273e-02,  3.5253e-02,  1.1895e-02, -1.5008e-02,  1.1340e-02,\n          8.7548e-03,  1.0551e-02, -8.2951e-03, -4.5204e-03,  1.7153e-02,\n          1.1433e-02,  1.0239e-02, -2.0005e-02, -1.6386e-02,  5.0513e-03,\n          2.1361e-02, -1.0189e-02,  1.6184e-02,  2.9124e-02,  1.8338e-02,\n          2.8306e-02,  1.7149e-02, -3.3173e-02,  1.9080e-02, -2.5537e-02,\n         -2.2043e-02, -3.2582e-02,  1.4720e-02,  3.7900e-03],\n        requires_grad=True),\n Parameter containing:\n tensor([[ 0.0113,  0.0337,  0.0374,  ...,  0.0149,  0.0041,  0.0355],\n         [-0.0282, -0.0096, -0.0432,  ...,  0.0066,  0.0465, -0.0468],\n         [ 0.0287, -0.0359,  0.0572,  ..., -0.0361, -0.0028, -0.0518],\n         ...,\n         [-0.0519,  0.0462, -0.0250,  ..., -0.0545,  0.0517,  0.0211],\n         [ 0.0518, -0.0566,  0.0276,  ..., -0.0192, -0.0427,  0.0614],\n         [-0.0399,  0.0163,  0.0559,  ...,  0.0420, -0.0151, -0.0350]],\n        requires_grad=True),\n Parameter containing:\n tensor([-1.6460e-02,  2.4588e-02,  1.8539e-02,  1.1956e-02, -2.1846e-02,\n         -1.3578e-02, -1.7915e-02, -1.4473e-03, -2.3006e-02, -1.9198e-02,\n         -3.4330e-02,  2.9714e-02,  3.6715e-03, -2.8976e-02, -2.9545e-02,\n          3.4345e-02, -1.5523e-02, -2.2418e-03,  1.9441e-02, -2.7478e-02,\n         -2.9531e-02,  1.1584e-02,  6.9170e-03, -2.2452e-02,  8.7714e-03,\n          2.3927e-02,  3.5215e-02,  2.8159e-02,  1.5122e-03, -1.6429e-02,\n         -2.5109e-02,  3.3294e-02,  2.6965e-02,  8.9711e-03, -1.9553e-02,\n          2.5058e-02,  1.8367e-02,  2.7014e-03,  1.6014e-02, -6.2850e-04,\n         -2.0577e-02, -3.3885e-02, -3.0648e-03,  2.5376e-02, -2.2609e-02,\n         -2.0448e-02,  1.3604e-02,  2.0841e-02,  2.5732e-02,  2.7150e-02,\n          3.4379e-02, -1.5972e-02,  3.4421e-02, -1.9214e-02,  6.1869e-03,\n          2.6976e-02,  1.5504e-02,  3.4016e-02,  1.1761e-02, -2.8880e-02,\n          6.2406e-03,  1.5392e-02,  1.4027e-02, -2.2799e-02,  7.7854e-03,\n         -1.0574e-02, -2.3439e-02, -2.7851e-02,  1.7097e-03, -2.2875e-02,\n          1.1420e-03, -4.6883e-03, -7.2000e-03,  2.1132e-02,  1.9846e-02,\n          1.1462e-02,  3.1970e-02,  5.3110e-03, -7.6879e-03, -2.5490e-02,\n         -3.0799e-02,  1.3400e-02, -2.6899e-02,  2.7887e-02,  6.6181e-03,\n          3.1803e-02, -1.4985e-02,  1.2449e-02, -9.0633e-03,  3.7326e-03,\n          1.5615e-02, -3.4760e-02, -3.4019e-02,  3.5104e-02,  4.0684e-03,\n          1.8208e-02, -1.0868e-02,  1.8349e-02,  1.7191e-02,  1.0263e-02,\n          2.7900e-02,  6.1344e-03,  3.0672e-02,  3.4168e-02,  2.9118e-02,\n          4.3957e-03, -3.0250e-02, -3.2802e-02,  2.7541e-02, -9.6755e-03,\n         -2.4473e-02, -2.2700e-02, -2.6796e-02,  2.9267e-02,  3.0974e-02,\n          2.7497e-03, -1.4935e-02,  1.3491e-02,  1.7304e-03, -1.1907e-02,\n         -8.5952e-03,  1.9100e-02,  3.3708e-02, -1.1958e-02, -7.2792e-03,\n          1.5615e-02,  3.2489e-04, -1.9004e-02,  3.5655e-02, -1.8459e-02,\n         -1.4397e-02, -1.5204e-02,  3.4436e-03,  6.1168e-03, -4.3729e-03,\n         -1.8391e-02, -3.1768e-02, -1.1916e-03,  4.2891e-03,  2.2443e-02,\n         -2.6947e-02,  2.4899e-02,  2.0801e-02, -2.5136e-02,  3.5633e-02,\n         -2.3568e-02, -3.0030e-02, -3.4318e-02, -1.0168e-02, -2.1499e-02,\n         -2.4316e-02, -2.5333e-02,  2.1461e-02,  2.2977e-02,  1.1741e-02,\n         -2.9338e-02, -3.4228e-02,  9.7225e-03,  8.6094e-03, -5.2541e-03,\n         -1.2846e-02, -2.3742e-02,  2.9199e-02, -6.8147e-03,  1.2269e-02,\n         -2.0415e-02,  3.4337e-02,  6.8180e-03, -1.3603e-02, -2.8069e-02,\n         -2.5107e-02, -2.5176e-02,  3.5245e-02,  6.1178e-03,  8.4667e-03,\n          1.2576e-02,  8.4724e-03, -2.8815e-02,  5.1523e-03,  1.1371e-02,\n          3.8675e-03, -4.7483e-03,  1.8481e-02, -3.5035e-04,  1.4664e-02,\n          1.9194e-03, -1.4558e-02,  2.7402e-02, -2.3378e-03,  3.8506e-03,\n         -2.4547e-02, -2.3865e-02, -2.1971e-02, -3.1903e-02, -1.6285e-02,\n         -2.9902e-02, -1.4499e-02,  1.9777e-02,  1.6145e-02,  2.1345e-02,\n          2.9926e-02,  3.1157e-04,  2.6391e-02,  2.7515e-02,  1.9832e-02,\n          2.8528e-03, -3.1231e-02, -2.1535e-02,  7.9272e-03,  3.1637e-02,\n         -7.7195e-03,  2.6741e-02,  1.7818e-02,  2.9072e-02,  1.4687e-02,\n         -2.7172e-02,  5.3638e-03,  1.7038e-03, -1.0968e-02,  3.3185e-02,\n         -8.7061e-03,  1.5823e-02,  9.7927e-04,  1.3277e-04, -3.9137e-03,\n          2.3306e-02, -1.1192e-02, -1.7092e-02, -1.8664e-02, -3.1175e-02,\n          2.9050e-02,  2.8115e-02,  2.9407e-02,  1.2924e-02, -2.3933e-03,\n         -2.2550e-02, -7.3615e-03, -2.2645e-02, -9.8632e-03,  5.6282e-03,\n         -2.1986e-03, -3.3104e-02, -3.2476e-02, -2.1625e-02, -6.3990e-03,\n         -3.1006e-03, -2.6813e-02, -5.5012e-03, -3.3472e-02, -7.6344e-03,\n          5.4781e-03, -9.2332e-03, -3.2226e-02, -2.3135e-02,  2.1255e-03,\n         -2.3514e-04,  1.6873e-02,  1.7017e-02,  2.5501e-02,  2.3462e-02,\n          1.7765e-02,  3.4927e-02, -1.5750e-02,  1.2347e-02,  3.1559e-02,\n         -1.9702e-02,  2.1079e-02, -6.8551e-03,  2.3340e-02,  7.2501e-04,\n          3.4093e-02, -2.1531e-02, -2.0059e-02,  3.4304e-03,  1.2858e-02,\n         -1.9421e-02,  1.4775e-02,  3.3207e-02, -2.2273e-02,  3.1021e-02,\n         -1.3440e-02,  1.5006e-02,  1.6134e-02, -8.7577e-03, -1.3848e-02,\n         -7.7334e-03,  3.3943e-02, -2.9623e-02, -2.9537e-02,  1.8285e-02,\n         -3.9049e-03,  6.1121e-03, -1.8870e-02,  1.5929e-02,  3.4636e-02,\n         -2.7437e-02, -3.1857e-02,  9.1331e-03,  1.8317e-02,  3.5669e-03,\n         -1.7122e-02,  6.8455e-03,  1.3375e-02, -1.8794e-02, -2.1188e-02,\n         -2.5810e-02,  1.4416e-02, -3.4422e-03,  1.6422e-02,  3.1371e-02,\n         -1.5812e-02, -2.8909e-02, -2.5673e-02, -9.1059e-03, -1.7956e-02,\n         -2.6995e-02, -1.5366e-03,  2.7325e-02,  2.5151e-02,  6.8975e-03,\n          2.3397e-02, -2.8224e-02, -2.2425e-03,  5.4871e-03,  6.6668e-03,\n         -1.2199e-02,  1.2438e-02,  1.3578e-02,  3.1863e-02,  4.3103e-03,\n         -1.5256e-03,  3.5521e-02,  2.1183e-03, -9.7969e-03,  1.4800e-04,\n          2.9346e-02, -5.9749e-03, -6.1675e-03, -2.2725e-02, -1.2866e-02,\n         -2.3274e-02, -2.6822e-02, -3.1115e-02, -1.7524e-02,  1.2786e-02,\n         -1.5282e-03, -2.2152e-02,  1.9830e-02, -1.9837e-02, -5.9010e-03,\n          1.4521e-02, -1.7865e-02, -9.6035e-03,  7.7702e-03, -2.1823e-02,\n         -7.1331e-03, -2.4265e-02,  3.2420e-02, -3.3160e-02, -6.3086e-03,\n          1.2420e-03,  2.6826e-02, -2.7827e-02, -1.2252e-02, -1.3889e-02,\n         -7.9653e-03,  1.0161e-02, -7.6507e-03,  2.1733e-02, -8.7453e-03,\n          1.6166e-02,  2.2164e-02, -3.4322e-02, -2.9702e-02, -2.3717e-02,\n         -3.3826e-02,  3.0539e-02, -2.7379e-02, -1.6716e-02,  5.2991e-03,\n          2.0623e-02, -2.0276e-02, -3.6347e-03, -6.4043e-03, -2.7714e-02,\n          1.8312e-02,  6.7032e-03, -3.2419e-02,  1.4015e-02,  1.2391e-02,\n          1.3832e-02, -1.3362e-02, -2.4368e-02, -4.2962e-03,  6.4947e-03,\n         -1.1373e-02, -1.9629e-02, -5.4331e-03, -2.7614e-02, -1.4973e-02,\n         -1.0054e-02,  2.1915e-02,  2.5528e-02, -2.1502e-02, -3.0164e-02,\n          2.3882e-02,  3.1025e-03, -2.7290e-02, -8.7382e-03, -2.5066e-02,\n         -1.6112e-02, -1.2159e-02, -1.7296e-02, -3.1773e-02,  2.4080e-02,\n         -5.1613e-03,  1.1427e-02,  3.5160e-02, -2.3781e-02,  2.7896e-02,\n         -2.7891e-02,  3.0071e-02, -1.0745e-02, -2.9325e-02, -1.3734e-02,\n          1.4508e-02,  2.5249e-02,  2.1181e-02, -1.2353e-02,  1.0596e-02,\n          4.1330e-03, -1.2017e-02, -3.5222e-02, -1.7785e-02, -8.6973e-03,\n          2.5020e-02,  9.3973e-03, -3.3983e-02,  2.9570e-02,  2.0840e-02,\n         -1.8197e-02, -6.4258e-03,  3.5352e-03,  6.2556e-03,  1.3878e-02,\n         -1.7321e-03, -3.5139e-02, -1.6960e-02,  1.3361e-02,  2.5560e-02,\n          1.7085e-02, -1.6935e-03, -7.8657e-03, -1.9663e-02, -1.5932e-02,\n          2.9983e-02,  2.3538e-02,  3.2523e-02,  1.6768e-02, -1.6020e-02,\n         -5.0969e-03, -9.4807e-03,  1.5458e-03,  2.1265e-02,  3.2636e-02,\n         -2.9077e-02, -1.6974e-04, -1.4020e-02,  2.1952e-02,  3.1458e-02,\n          3.3554e-02,  2.2475e-02,  1.7535e-02,  3.3113e-02,  2.0984e-02,\n         -9.1607e-03,  2.6111e-02,  2.8645e-04, -1.9049e-02, -8.8537e-03,\n         -6.9969e-03, -2.6876e-02, -2.6615e-02,  6.7779e-03,  1.8738e-02,\n         -6.7290e-04, -2.8752e-03, -2.6107e-02,  1.2674e-02,  3.3660e-02,\n          2.9540e-02, -6.3733e-04,  2.4919e-02, -2.1644e-02,  2.2670e-02,\n         -3.5295e-02, -4.5469e-03, -3.5510e-02, -6.3171e-03,  1.8226e-02,\n         -3.3106e-02,  1.2060e-02,  1.8170e-02,  2.0788e-02,  2.8180e-02,\n         -2.2356e-02, -1.9356e-03, -1.2173e-02, -2.8247e-02, -1.9812e-02,\n          1.0650e-03,  1.6103e-02, -3.4689e-02, -1.2692e-02,  3.5009e-02,\n          7.9030e-03,  3.4841e-02,  1.8303e-02,  1.8699e-02,  1.4376e-02,\n         -1.5062e-02, -9.6139e-05, -2.0738e-02,  2.2738e-02,  1.6352e-02,\n          2.4496e-02, -1.3613e-02,  2.7011e-02,  3.4540e-02, -4.6495e-03,\n          3.5027e-02, -2.4513e-02, -1.1833e-02, -1.2901e-02, -3.4784e-03,\n         -9.9403e-03,  2.4356e-02,  5.1475e-03,  2.1710e-02,  2.4159e-02,\n          3.3641e-02,  4.3251e-03, -1.6340e-02, -6.3930e-03, -2.6856e-03,\n         -1.8122e-02,  1.3946e-02, -1.8062e-02,  3.1418e-02,  3.1769e-02,\n         -3.3449e-02,  3.5565e-02, -2.5274e-02,  2.6579e-02, -1.0345e-02,\n          3.2174e-02, -1.0218e-02, -2.3267e-03,  1.8681e-02,  2.2104e-02,\n         -4.6470e-03,  8.3551e-04, -1.9295e-02, -4.8832e-03, -1.5409e-04,\n         -1.9825e-02, -4.6104e-03,  1.4733e-02,  3.2229e-02, -1.5727e-02,\n         -3.1516e-02, -3.0423e-02,  7.8059e-03, -9.8433e-03,  2.1254e-02,\n          1.4651e-02, -3.8527e-03,  2.2464e-02,  3.4183e-02, -3.3648e-02,\n         -1.7389e-02, -1.0736e-02, -5.7507e-04, -2.4058e-02,  1.1963e-02,\n          1.1937e-02, -1.6076e-02, -3.5301e-02,  1.4332e-02,  8.7180e-03,\n         -9.4008e-03,  2.5144e-02, -1.2762e-02,  4.1385e-03, -2.6932e-03,\n         -2.0518e-03, -7.2566e-04,  1.1603e-04,  1.1648e-02, -9.0419e-03,\n         -3.2566e-02, -1.0204e-02, -3.1941e-02,  1.2826e-03,  2.4835e-02,\n          4.9533e-03,  1.7109e-02, -1.0347e-02,  3.3121e-02,  1.8321e-02,\n          2.5415e-02,  2.4889e-02,  1.4777e-02,  8.1917e-03,  2.5152e-02,\n          2.6042e-02, -1.3924e-02, -5.6110e-05, -2.2818e-02,  1.3320e-02,\n         -2.2748e-02, -3.4884e-02,  4.9163e-03,  2.4861e-02, -2.4318e-03,\n         -5.3486e-03,  1.5522e-02,  4.8772e-03, -1.1428e-02,  2.2877e-02,\n         -1.2084e-02, -3.0410e-02, -3.4605e-02, -2.6605e-02,  2.9219e-02,\n          2.9245e-02, -4.3731e-03,  1.1140e-02, -8.1309e-03, -3.5081e-02,\n          6.5439e-03, -2.7523e-02, -1.6976e-02,  1.1862e-02, -9.5229e-03,\n          1.5986e-02, -2.4757e-02,  3.4417e-02, -2.3504e-02, -2.4633e-02,\n          4.3211e-03, -1.7869e-02, -2.4517e-02, -2.0345e-02,  2.1420e-02,\n         -1.1096e-02,  1.5050e-02,  5.5998e-03, -2.8727e-02, -1.3398e-02,\n         -3.1742e-02, -1.9202e-02,  2.7990e-02, -2.4624e-02,  2.7113e-02,\n          6.6164e-03, -4.8772e-04, -3.0557e-02,  2.8051e-02, -2.2814e-02,\n         -1.2381e-02, -3.2276e-02, -1.0598e-02,  7.9914e-03, -1.4200e-02,\n          3.1500e-02, -5.0623e-03, -3.9852e-03, -2.3423e-02,  3.2739e-02,\n         -3.3471e-02, -9.6738e-05,  3.5195e-02,  4.2204e-03,  3.4915e-02,\n          2.2062e-02, -1.7228e-02, -1.6562e-02, -3.1426e-02, -8.9576e-04,\n         -7.2846e-03, -3.1876e-02, -2.2992e-02, -5.1246e-03, -9.3189e-03,\n         -3.4427e-02,  1.7934e-02,  5.0507e-03,  1.6990e-02, -3.3912e-02,\n          6.2052e-03,  8.5230e-03, -2.7671e-02,  1.3399e-02, -2.4033e-02,\n          4.7850e-03, -1.8727e-02, -1.0741e-02, -1.2634e-02,  1.2879e-02,\n          8.5359e-03, -8.7973e-03,  1.3507e-02,  6.2138e-03, -2.4023e-02,\n         -1.2151e-02,  3.0345e-02,  2.5253e-03,  1.4920e-02,  1.4452e-02,\n         -3.5061e-02,  1.2247e-02, -3.0280e-02, -1.2909e-02, -6.7008e-03,\n         -1.7399e-02, -2.6646e-02,  1.6824e-02, -6.5902e-03, -1.6077e-02,\n         -1.2227e-02, -2.6326e-02, -2.6691e-02,  4.1919e-03,  2.2119e-02,\n          9.6678e-03,  2.3751e-02, -1.1763e-02,  1.5398e-02, -2.8851e-02,\n          3.3154e-02, -2.5998e-02, -1.8976e-02,  2.7020e-02,  3.5183e-02,\n          2.6858e-02, -1.0891e-02,  1.9763e-02, -2.8037e-02,  9.6072e-03,\n         -8.7652e-04, -7.4662e-03,  1.3170e-02,  1.2293e-04, -4.9000e-03,\n          1.9168e-02,  1.5061e-02, -3.0066e-03, -1.3189e-02,  1.6673e-02,\n         -1.4286e-02,  1.3356e-02,  2.5556e-02,  5.9048e-03, -2.6083e-02,\n         -2.4861e-02, -1.5019e-02,  7.6443e-03, -1.4430e-02, -8.1154e-03,\n          3.5528e-02, -5.8955e-03, -4.0146e-03, -2.4581e-02, -2.0100e-02,\n         -6.4839e-03,  3.2515e-02,  1.1015e-02,  3.2173e-02, -4.6436e-03,\n          6.1037e-03, -2.1143e-02,  8.3793e-05, -1.1934e-02],\n        requires_grad=True),\n Parameter containing:\n tensor([[-0.0155,  0.0390, -0.0344,  ...,  0.0418,  0.0351, -0.0100],\n         [-0.0056,  0.0080, -0.0447,  ..., -0.0178, -0.0414, -0.0324],\n         [ 0.0007, -0.0461,  0.0143,  ..., -0.0272, -0.0153,  0.0144],\n         ...,\n         [-0.0020, -0.0057, -0.0454,  ...,  0.0600, -0.0437,  0.0462],\n         [-0.0035,  0.0572, -0.0532,  ...,  0.0133,  0.0552,  0.0196],\n         [ 0.0226,  0.0500, -0.0323,  ...,  0.0197, -0.0542, -0.0098]],\n        requires_grad=True),\n Parameter containing:\n tensor([ 8.6728e-05, -1.9129e-02, -2.7950e-02, -5.3378e-03, -3.0204e-02,\n         -1.9210e-03, -3.1888e-02, -1.5432e-02,  3.0363e-02,  2.0433e-02,\n          2.8036e-02, -8.2703e-03,  2.0658e-02, -2.7402e-02,  1.9878e-02,\n          1.2144e-02, -8.7386e-03, -2.6481e-02,  1.1570e-02,  2.6885e-02,\n          2.0986e-02,  3.3181e-02, -2.4984e-04,  2.2191e-02,  5.8948e-03,\n         -1.2087e-02,  3.0282e-02, -8.4294e-03, -7.6944e-03, -9.7913e-03,\n         -3.4345e-02,  1.9524e-04,  7.6635e-03, -7.6400e-03, -1.5031e-02,\n          7.2232e-03, -1.1613e-02, -4.9556e-03, -3.4105e-02,  3.1557e-02,\n         -3.4619e-02,  1.1426e-03,  3.5454e-02,  4.1999e-03,  3.3433e-02,\n          2.1757e-02, -7.0482e-03, -2.1852e-02, -3.1743e-02, -1.1495e-02,\n         -2.6602e-02, -9.8592e-03,  1.8734e-02, -2.3536e-02,  2.1458e-02,\n          1.7249e-03,  2.1508e-02,  1.9828e-02,  2.8990e-02, -3.0379e-02,\n         -2.4634e-03, -2.0880e-02,  1.9603e-02,  6.2726e-03, -2.7481e-02,\n          7.1379e-03, -7.0375e-03, -9.9617e-03, -2.0093e-02, -4.6247e-04,\n         -6.9451e-03,  2.3362e-03, -2.4406e-02, -4.2710e-03, -5.4342e-03,\n         -2.1410e-02, -1.5768e-03,  3.2365e-02, -7.5735e-04,  1.1760e-02,\n         -4.9549e-03,  6.5017e-04, -2.1383e-02, -1.7143e-02, -1.4731e-02,\n         -1.1752e-02,  3.2831e-03, -1.7272e-02,  2.2599e-02,  5.9080e-03,\n          1.3964e-02, -1.9003e-02, -1.1307e-02,  1.5465e-02,  2.2064e-02,\n          3.0983e-02, -8.7927e-03, -2.0165e-02,  2.4276e-02, -2.1379e-02,\n          2.2105e-02,  2.3463e-02,  2.4044e-02, -3.5060e-02,  3.5085e-02,\n          1.7809e-02, -2.2073e-02,  2.1990e-02,  8.6096e-03, -2.4235e-02,\n         -5.1080e-03,  1.2985e-02,  1.8473e-02,  2.2508e-02, -2.8877e-02,\n         -3.0026e-02,  1.1578e-02,  3.2334e-02, -3.1119e-02,  3.1132e-02,\n         -1.3044e-02, -1.6949e-02,  2.9972e-02,  4.2591e-03,  4.4082e-03,\n         -1.2729e-02,  2.8006e-02, -1.1087e-02,  1.6580e-02, -1.0280e-02,\n         -1.7824e-02, -3.4855e-02,  2.5650e-02,  2.0838e-02, -3.4555e-02,\n          3.0558e-02, -2.9562e-02,  1.3546e-02, -3.3063e-02,  2.4363e-02,\n         -8.5385e-03, -5.6210e-03, -3.2480e-02, -1.0318e-02, -2.4398e-02,\n          1.5429e-02,  3.2646e-02,  8.9506e-04, -2.9264e-02,  2.1722e-03,\n          1.5433e-02,  1.2195e-02,  2.7473e-03,  2.3725e-02,  4.8595e-03,\n         -7.2267e-03, -1.1908e-02,  1.1932e-02,  6.6474e-03,  2.6224e-02,\n          2.7188e-02,  1.0767e-02,  1.1841e-02,  3.3076e-02, -2.0563e-02,\n         -2.4268e-02, -2.2838e-02,  2.8058e-02,  1.5256e-02,  1.7763e-02,\n         -1.4962e-02,  1.1478e-02, -1.7913e-02,  3.2235e-02, -8.3659e-03,\n         -3.2189e-03, -8.0137e-04, -1.0868e-02, -8.6651e-03,  2.9062e-02,\n         -3.1099e-02, -9.2856e-03, -7.5850e-03,  2.9264e-03, -2.9739e-02,\n         -2.3734e-03,  1.0517e-02,  1.9512e-02,  3.2616e-02, -2.4004e-02,\n         -2.1079e-02,  1.8781e-02,  1.7002e-02,  1.9015e-02, -2.3299e-02,\n         -6.6442e-03,  6.3227e-03, -2.7093e-02,  1.2349e-02, -1.1283e-02,\n         -2.6415e-02,  1.1575e-02, -8.7415e-03,  3.1354e-02, -4.9101e-03,\n         -1.9060e-02, -3.0838e-02, -2.3355e-02, -4.6373e-03,  2.2482e-02,\n         -3.6479e-03, -2.9873e-02, -3.1279e-02,  1.4544e-02, -2.5271e-02,\n          2.0568e-02,  2.9337e-02,  2.2705e-02,  1.8896e-02, -1.2510e-02,\n          6.0725e-03, -7.4537e-04,  3.4443e-02, -2.3166e-02, -2.9654e-02,\n          1.3048e-02,  1.9856e-02,  4.8190e-03,  3.3174e-03,  1.3248e-02,\n          8.9287e-03, -9.3331e-03,  1.5777e-03,  2.5982e-02,  1.1053e-02,\n          1.1722e-02,  2.9039e-02,  1.6062e-02,  6.5284e-03, -3.4481e-02,\n          3.5140e-02, -6.9194e-03, -1.2643e-02, -2.1613e-03, -5.0803e-03,\n          1.8496e-02,  1.0623e-02,  5.1439e-04,  2.4544e-02, -6.9685e-03,\n          3.6718e-03, -1.3199e-03,  3.1345e-02, -2.3550e-02, -2.5139e-02,\n          1.3419e-03, -1.0333e-02, -4.0198e-03, -1.4565e-02, -8.1088e-03,\n          1.9351e-02,  4.5845e-03, -2.2171e-02,  2.9636e-02, -2.9936e-02,\n          1.9263e-02, -5.9207e-03,  2.4692e-03, -8.3065e-03, -7.5952e-03,\n         -1.1304e-02,  1.4325e-02, -8.6289e-03, -3.0605e-02,  7.8743e-03,\n          9.0902e-03, -3.1221e-02,  3.4756e-02,  7.1761e-03,  5.6890e-03,\n         -2.2624e-02, -6.3534e-03,  7.0813e-03, -1.7915e-02,  1.7717e-02,\n          8.1092e-03,  3.2970e-02, -1.2123e-02,  2.3675e-02,  6.5377e-03,\n         -1.0230e-02,  1.0952e-02, -8.1560e-03, -4.3015e-03,  2.7093e-03,\n          2.6575e-02, -3.4237e-02, -1.6594e-02, -1.8161e-02, -4.7344e-03,\n         -1.8144e-03,  6.4406e-04, -5.5193e-03, -1.0427e-02, -1.4950e-02,\n          3.0196e-02,  1.0580e-03,  1.5363e-02,  2.5997e-02, -1.0937e-02,\n         -1.8486e-02,  1.3773e-02, -6.0790e-03, -1.6719e-03,  1.0666e-02,\n          9.6932e-03,  2.3646e-02,  3.4145e-02,  7.8167e-03,  8.8987e-03,\n         -3.4826e-02,  2.3906e-02, -1.4330e-02, -2.1381e-02,  3.3389e-02,\n          3.1749e-02, -9.8836e-03, -1.8858e-02,  3.2723e-03,  1.0873e-02,\n         -1.0532e-02,  6.7436e-03,  2.4455e-02,  1.4075e-03, -1.8077e-02,\n          3.3986e-02,  2.8651e-02,  3.2147e-02, -1.4185e-02,  1.3304e-02,\n         -3.2556e-02, -9.3219e-03, -2.2869e-02,  2.4808e-02, -2.7312e-02,\n         -1.2125e-02, -1.6203e-02, -3.3895e-02,  1.4114e-02, -1.4215e-02,\n          3.4270e-02, -2.0167e-03,  3.2113e-02, -1.3179e-02,  1.4553e-02,\n          1.3827e-02, -1.7397e-02, -2.8085e-02, -1.6696e-04, -3.0440e-02,\n          1.5764e-02,  2.0309e-02, -5.7793e-03, -1.3958e-02,  2.4846e-02,\n          5.2947e-03, -2.5515e-02,  2.8520e-02,  2.9159e-02,  4.8745e-03,\n         -5.9980e-03, -7.6045e-03,  7.9953e-03, -3.2501e-02, -9.6653e-03,\n         -1.4354e-02, -7.5430e-04, -1.4534e-02, -1.5789e-02,  1.7123e-02,\n          2.5780e-02,  1.7805e-02,  3.2778e-02, -1.2546e-02, -8.6597e-04,\n          3.3044e-02,  1.5078e-02, -1.0068e-02, -3.3760e-02,  3.3155e-02,\n         -1.4069e-02,  2.5643e-02,  1.8336e-02, -1.5014e-02, -2.7827e-02,\n         -3.8391e-03, -2.5312e-02, -3.0428e-02, -9.0911e-03, -3.3525e-02,\n          1.5087e-02,  2.5380e-02,  3.4274e-02, -2.7317e-02,  1.5982e-02,\n         -9.6739e-03,  1.9576e-02, -3.4539e-02, -2.8664e-02, -1.5845e-02,\n          2.4068e-02,  1.1306e-02,  2.3615e-02, -2.6724e-02, -1.5831e-02,\n         -2.4466e-03,  3.0222e-02, -9.8443e-03,  2.5140e-02, -5.2928e-03,\n          8.5784e-03,  8.5921e-03, -4.9690e-03, -5.2806e-03,  2.1909e-02,\n          1.4851e-02, -2.8305e-02, -2.5275e-02, -1.2864e-03, -8.3060e-03,\n          1.5898e-02,  7.3033e-03,  3.1583e-02, -9.6767e-03, -2.2316e-02,\n         -2.3897e-03,  1.6015e-03,  3.0475e-02,  1.7752e-03, -3.2563e-02,\n          1.1072e-02, -2.2355e-02,  3.1796e-02,  2.9447e-02,  1.2857e-02,\n         -6.6730e-03, -2.9359e-02, -3.4163e-02, -1.1260e-02, -3.2843e-02,\n         -2.0254e-02, -1.3222e-02, -2.6475e-02,  3.4190e-02,  4.2639e-04,\n         -3.4756e-02,  9.2756e-03, -2.7670e-02,  3.2134e-03, -1.0730e-02,\n          2.6755e-02, -2.6633e-02,  2.3112e-02,  2.8462e-03, -1.2107e-02,\n         -1.5192e-03,  3.0487e-02,  3.2736e-02,  1.9296e-02, -1.3241e-02,\n          1.1966e-02, -2.3173e-02, -1.5565e-02,  8.2411e-03,  3.1037e-02,\n          2.7210e-03,  3.0802e-02,  1.7941e-02, -9.0938e-03, -2.2813e-02,\n         -1.5106e-02,  2.9941e-02,  1.1610e-02, -2.9272e-02, -1.2885e-02,\n         -2.1834e-02,  2.3013e-02, -8.2188e-03, -2.1103e-02,  1.2412e-02,\n          1.1239e-02,  2.0798e-02,  1.5397e-02,  6.5415e-03,  1.5042e-02,\n         -3.2224e-02, -1.4508e-02, -2.1048e-02, -2.7389e-02,  1.0423e-02,\n         -2.6130e-02, -2.6666e-02,  5.3632e-03,  2.8580e-02,  1.2385e-03,\n          1.7869e-02, -1.4823e-02, -2.1274e-02,  2.4081e-03,  2.2800e-02,\n          3.4035e-02,  1.0445e-02, -9.7813e-03, -3.1400e-02, -1.1033e-02,\n         -2.4111e-03, -2.6364e-02, -8.3065e-04, -3.4307e-02, -1.9703e-02,\n         -2.4190e-02,  1.0189e-02,  1.4736e-02,  3.5243e-02,  2.5489e-02,\n         -2.4312e-02, -1.4214e-02, -3.1292e-02,  3.2015e-02,  1.1251e-02,\n         -1.0670e-02,  1.5454e-02,  1.0855e-02,  3.9535e-04, -1.7072e-02,\n          7.9670e-03, -3.2687e-02,  3.4593e-02,  1.8893e-02,  1.4309e-02,\n          2.5877e-02, -1.5376e-02,  1.2498e-03,  3.4267e-02,  2.3796e-02,\n         -1.9692e-02,  2.3457e-02, -1.9129e-02,  1.9566e-02, -2.6430e-02,\n          3.5223e-02, -2.3710e-02, -2.1427e-02, -1.3811e-02, -2.8009e-02,\n          2.7339e-02,  1.3588e-02, -2.9475e-02, -3.0528e-02, -1.8240e-02,\n         -1.3850e-02, -1.3903e-02,  6.5720e-03, -3.1865e-02, -7.2510e-03,\n          2.5499e-02,  1.5267e-02, -1.1168e-02, -9.7737e-03,  4.9399e-03,\n          2.8212e-02,  2.9737e-02, -2.6606e-02, -2.9206e-02,  1.4097e-02,\n          2.6938e-02, -2.6063e-02,  3.4181e-02, -2.6550e-03, -1.1245e-02,\n         -2.3151e-02,  2.2780e-02,  8.6622e-03, -1.2216e-02, -2.6338e-02,\n         -3.4705e-02, -5.0887e-03, -7.4978e-03, -8.0565e-03,  1.3823e-03,\n          6.6440e-03, -1.7949e-02, -1.4479e-02, -2.5092e-02,  2.1313e-02,\n          1.4655e-03,  2.2531e-02, -2.1463e-02,  3.0863e-02,  2.2049e-02,\n         -1.4497e-02, -5.3207e-03, -9.9368e-03,  1.9867e-02,  5.4318e-03,\n          1.0564e-02,  2.8685e-02, -3.7318e-03,  1.7900e-02, -7.8651e-03,\n         -3.6789e-04, -2.1999e-02,  8.2940e-04,  3.2776e-02,  2.1920e-04,\n         -2.7551e-04,  7.5036e-03,  1.3874e-02,  2.3327e-02, -2.1735e-03,\n         -1.5753e-02, -2.2735e-02,  3.3912e-02,  1.7550e-02,  2.5114e-02,\n          8.9651e-03,  2.1123e-03,  1.8297e-02,  4.6137e-03,  1.1416e-02,\n          2.9259e-02,  4.2821e-03,  2.2063e-02,  3.5421e-02,  1.1415e-02,\n          1.1287e-02,  6.4977e-03, -6.0947e-03,  2.1060e-02, -2.3408e-02,\n          5.8000e-03,  2.3606e-02,  5.9356e-03, -1.6638e-02, -2.7446e-02,\n         -1.7298e-03,  8.2000e-03, -2.2748e-02, -1.7580e-02,  6.8860e-03,\n         -1.6489e-03, -6.9812e-03,  2.0198e-02,  1.3844e-03,  7.1620e-04,\n          1.2146e-02,  1.3661e-02,  5.4681e-03,  3.0044e-02, -4.2005e-03,\n         -8.8047e-03,  1.2301e-02,  1.3770e-02,  1.3232e-03, -1.6926e-02,\n         -2.4998e-02,  1.8533e-02,  1.6831e-03, -1.6024e-02, -2.1617e-03,\n         -2.6900e-02,  3.4375e-02, -8.3134e-04, -9.4687e-03, -5.3460e-03,\n         -8.0147e-03, -1.8141e-02, -1.9282e-02, -1.7918e-02,  7.7668e-03,\n          3.2638e-02,  3.0046e-02,  2.5187e-02,  1.2147e-02, -6.2698e-03,\n          1.7044e-03,  2.0124e-02,  8.8839e-03,  4.8876e-03,  2.7401e-02,\n         -3.0286e-02, -1.4615e-02,  2.3529e-02, -2.5329e-02, -2.9917e-02,\n         -3.3325e-02, -3.4565e-02, -1.2997e-02, -2.0607e-02, -3.1426e-02,\n          3.4575e-03, -3.0371e-02, -3.3453e-02,  1.8907e-02, -1.3094e-02,\n         -2.9717e-02,  1.2249e-02,  3.4522e-02, -3.1817e-02, -1.4731e-02,\n         -2.2249e-02,  2.4036e-02,  4.0600e-03, -4.7188e-03,  1.5729e-02,\n         -6.2328e-03,  5.0588e-03, -1.1403e-03, -9.2031e-03,  2.4629e-02,\n          1.0278e-02, -4.4211e-03,  2.8730e-02,  2.9213e-02,  2.7612e-02,\n          9.9251e-03,  1.8361e-02,  5.4767e-03, -1.1660e-02, -5.8824e-03,\n          1.1994e-02,  2.6281e-02,  2.1956e-02,  4.3410e-03,  2.5339e-02,\n          2.3152e-03,  1.6822e-02, -2.9212e-02,  2.9052e-02, -2.5391e-02,\n         -1.5457e-02, -1.6186e-03,  3.3026e-02, -2.7683e-02,  1.4762e-02,\n         -1.8699e-02, -3.2752e-02, -1.7195e-02, -1.0123e-02, -2.0611e-02,\n         -1.0492e-02, -8.7873e-03, -1.7376e-02, -1.4268e-02, -2.0843e-02,\n          1.0097e-03,  2.6646e-02, -6.2121e-03, -3.1603e-02,  1.5245e-02,\n         -3.5109e-02,  1.0665e-02,  1.7412e-02,  3.5249e-02, -3.1166e-02,\n         -2.0859e-02, -3.4248e-02,  3.5518e-03, -1.0341e-02,  1.4324e-02,\n         -5.9402e-03,  3.5420e-02,  2.2247e-02,  1.5690e-02,  2.5031e-02,\n         -3.5438e-02, -2.9123e-02,  2.2761e-03, -1.7521e-02, -6.9755e-03,\n         -3.5343e-02, -1.9576e-02, -3.3687e-02, -1.3115e-02],\n        requires_grad=True),\n Parameter containing:\n tensor([[ 0.0008, -0.0118, -0.0621,  ...,  0.0346, -0.0151, -0.0765],\n         [ 0.0075, -0.0375,  0.0416,  ..., -0.0745, -0.0791,  0.0149],\n         [ 0.0292,  0.0589,  0.0077,  ...,  0.0106, -0.0118, -0.0676],\n         ...,\n         [-0.0747,  0.0707,  0.0663,  ..., -0.0587, -0.0659,  0.0747],\n         [ 0.0583,  0.0701,  0.0448,  ...,  0.0506,  0.0666,  0.0382],\n         [-0.0119, -0.0420,  0.0482,  ..., -0.0077, -0.0858,  0.0260]],\n        requires_grad=True),\n Parameter containing:\n tensor([-0.0178,  0.0021,  0.0223, -0.0073,  0.0226, -0.0119,  0.0026,  0.0347,\n         -0.0316,  0.0155], requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True),\n Parameter containing:\n tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n Parameter containing:\n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        requires_grad=True)]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mlp.parameters())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 3.1042, Train Acc: 0.1124, Test Acc: 0.1135\n",
      "Epoch 2/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 3/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 4/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m mlp,results \u001B[38;5;241m=\u001B[39m \u001B[43mwarm_up\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmlp\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtestloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\utils\\trainig_testing.py:154\u001B[0m, in \u001B[0;36mwarm_up\u001B[1;34m(model, trainloader, testloader, num_epochs)\u001B[0m\n\u001B[0;32m    150\u001B[0m correct \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, wl, targets, ind \u001B[38;5;129;01min\u001B[39;00m trainloader:\n\u001B[1;32m--> 154\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    156\u001B[0m     loss,new_labels \u001B[38;5;241m=\u001B[39m partial_loss(outputs,partial_weight[ind,:]\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mdetach(),\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    157\u001B[0m     partial_weight[ind,:] \u001B[38;5;241m=\u001B[39m new_labels\u001B[38;5;241m.\u001B[39mclone()\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\models\\model.py:33\u001B[0m, in \u001B[0;36mMLP.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# Iterate over the linear layers and apply them sequentially to the input\u001B[39;00m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 33\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn:\n\u001B[0;32m     35\u001B[0m             x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_norms[i](x)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "mlp,results = warm_up(mlp,trainloader,testloader,num_epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
