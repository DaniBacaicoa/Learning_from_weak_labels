{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quick example of Supervised\n",
    "## with CE and using the Moore Penrose inverse as the Virtual Label matrix\n",
    "This example tries to show the usage of the classes and methods\n",
    "proposed here to show classification with weakly labeled datasets.\n",
    "In this case we use the **CE** loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Importing general libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "# Importing specific classes and functions\n",
    "from utils.weakener import Weakener\n",
    "\n",
    "#Importing drawing tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing dataloaders\n",
    "from datasets.openml_datasets import OpenML_Dataset\n",
    "from datasets.torch_datasets import Torch_Dataset\n",
    "from models.model import MLP\n",
    "from utils.trainig_testing import train_model,evaluate_model,train_and_evaluate\n",
    "from utils.losses import EMLoss, CELoss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# DS stores the dataset and its related attributes\n",
    "DS = Torch_Dataset('mnist')\n",
    "train_x, train_y, test_x, test_y = DS.get_data()\n",
    "# WL stores pocesses relative to the Weakening process\n",
    "WL = Weakener(DS.num_classes)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Generation of the mixing matrix according to the model\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#  we've chosen.\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[43mWL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_M\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msupervised\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(WL\u001B[38;5;241m.\u001B[39mM)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(WL\u001B[38;5;241m.\u001B[39mlabels)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\utils\\weakener.py:152\u001B[0m, in \u001B[0;36mWeakener.generate_M\u001B[1;34m(self, model_class, alpha, beta)\u001B[0m\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;66;03m# [TBD]\u001B[39;00m\n\u001B[0;32m    151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _\n\u001B[1;32m--> 152\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mZ, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels \u001B[38;5;241m=\u001B[39m \u001B[43mlabel_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mM\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\utils\\utils_weakener.py:19\u001B[0m, in \u001B[0;36mlabel_matrix\u001B[1;34m(M, give_M)\u001B[0m\n\u001B[0;32m     16\u001B[0m e \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m c):\n\u001B[1;32m---> 19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mz_row\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m:\n\u001B[0;32m     20\u001B[0m         b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbin\u001B[39m(i)[\u001B[38;5;241m2\u001B[39m:]\n\u001B[0;32m     21\u001B[0m         b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m*\u001B[39m (c \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(b)) \u001B[38;5;241m+\u001B[39m b  \u001B[38;5;66;03m# this makes them same length\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "# Generation of the mixing matrix according to the model\n",
    "#  we've chosen.\n",
    "WL.generate_M(model_class='supervised')\n",
    "print(WL.M)\n",
    "print(WL.labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# We generate the weak labels (this step is optional as WL.virtual_labels(train_y) can do it)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m z, w \u001B[38;5;241m=\u001B[39m \u001B[43mWL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_weak\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_y\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\utils\\weakener.py:157\u001B[0m, in \u001B[0;36mWeakener.generate_weak\u001B[1;34m(self, y, seed)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_weak\u001B[39m(\u001B[38;5;28mself\u001B[39m, y, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;66;03m# It should work with torch\u001B[39;00m\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# the version of np.random.choice changed in 1.7.0 that could raise an error-\u001B[39;00m\n\u001B[1;32m--> 157\u001B[0m     d, c \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;66;03m# [TBD] include seed\u001B[39;00m\n\u001B[0;32m    159\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mz \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor([np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(d, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mM[:, tl]) \u001B[38;5;28;01mfor\u001B[39;00m tl \u001B[38;5;129;01min\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mmax(y, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m1\u001B[39m]])\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mint32)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# We generate the weak labels (this step is optional as WL.virtual_labels(train_y) can do it)\n",
    "z, w = WL.generate_weak(train_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "z is the numerical encodig of the weak label while w is a one-hot\n",
    "representation of that variable. z also encodes the row of the M matrix\n",
    "for that given weak label.\n",
    "\n",
    "Let's consider the example of partial label learning for the iris dataset.\n",
    "labels are encoded as ```{0: '011', 1: '101', 2: '110', 3: '111'}```\n",
    "so having an isntance with `z=3` means `w=[1,1,1]`, i.e., the weak label contains every label."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We generate the weak labels (this step is optional as WL.virtual_labels(train_y) can do it)\n",
    "WL.virtual_matrix(convex=True)\n",
    "#And we generate the virtual labels\n",
    "WL.virtual_labels(train_y)\n",
    "WL.v[:10,:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#We need to include this Weak Labels into the dataset,\n",
    "# we need to include w to mantain the coherence. z will only be used\n",
    "# in the loss for the EM\n",
    "DS.include_weak(WL.v)\n",
    "# Once we have our dataset with all the labels we need we can create the dataloaders.\n",
    "trainloader, testloader = DS.get_dataloader()\n",
    "\n",
    "print(len(testloader.dataset))\n",
    "print(len(trainloader.dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have a trainloader containing (X,v,y) and a testloader containig (X,y)\n",
    "so we can just establish our model and train it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of the MLP class\n",
    "mlp = MLP(input_size=DS.num_features, hidden_sizes=[20],\n",
    "          output_size=DS.num_classes, dropout_p=0.5)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = CELoss()\n",
    "\n",
    "train_losses,train_accs=train_model(mlp,trainloader,optimizer,\n",
    "                                    loss_fn,num_epochs=10)\n",
    "\n",
    "# Print the final training loss and accuracy\n",
    "print('Final Training Loss: {:.4f}'.format(train_losses[-1]))\n",
    "print('Final Training Accuracy: {:.4f}'.format(train_accs[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2)\n",
    "\n",
    "# For Sine Function\n",
    "axis[0].plot(train_losses)\n",
    "axis[0].set_title(\"Train_loss\")\n",
    "\n",
    "# For Cosine Function\n",
    "axis[1].plot(train_accs)\n",
    "axis[1].set_title(\"Train_accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an instance of the MLP class\n",
    "mlp = MLP(input_size=DS.num_features, hidden_sizes=[20],\n",
    "          output_size=DS.num_classes, dropout_p=0.5)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "loss_fn = CELoss()\n",
    "\n",
    "# Train and evaluate the MLP on the training data, test data, and get the results\n",
    "mlp, results = train_and_evaluate(mlp, trainloader, testloader,\n",
    "                                  optimizer, loss_fn, num_epochs=10)\n",
    "\n",
    "# Print the results\n",
    "print('Train Loss:', results['train_loss'][-1])\n",
    "print('Train Accuracy:', results['train_acc'][-1])\n",
    "print('Test Accuracy:', results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1, 2)\n",
    "\n",
    "# For Sine Function\n",
    "axis[0].plot(results['train_loss'])\n",
    "axis[0].set_title(\"Train_loss\")\n",
    "\n",
    "# For Cosine Function\n",
    "axis[1].plot(results['train_acc'])\n",
    "axis[1].plot(results['test_acc'])\n",
    "axis[1].set_title(\"Accuracy\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}