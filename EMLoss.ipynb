{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import datasets.datasets as dtset\n",
    "import utils.losses as losses\n",
    "\n",
    "from utils.weakener import Weakener\n",
    "from models.model import MLP\n",
    "\n",
    "from utils.trainig_testing import train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = dtset.Torch_Dataset('mnist', batch_size = 16)\n",
    "Weak = Weakener(Data.num_classes)\n",
    "Weak.generate_M(model_class='pll',pll_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([335, 529,  42,  ...,  47,   9,   9], dtype=torch.int32),\n",
       " tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [1., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X,train_y,test_X,test_y =  Data.get_data()\n",
    "Weak.generate_weak(train_y) #z and w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pickle\\nDataset = [Data,Weak]\\nf = open(\"Experimental_results/Datasets.pkl\",\"wb\")\\npickle.dump(Dataset,f)\\nf.close()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pickle\n",
    "Dataset = [Data,Weak]\n",
    "f = open(\"Experimental_results/Datasets.pkl\",\"wb\")\n",
    "pickle.dump(Dataset,f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weak.z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depending the model we are about to feed we will need either weak labels or virtual labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danibacaicoa\\vscode_projects\\Learning_from_weak_labels\\.venv\\lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:336: FutureWarning: \n",
      "    Your problem is being solved with the ECOS solver by default. Starting in \n",
      "    CVXPY 1.5.0, Clarabel will be used as the default solver instead. To continue \n",
      "    using ECOS, specify the ECOS solver explicitly using the ``solver=cp.ECOS`` \n",
      "    argument to the ``problem.solve`` method.\n",
      "    \n",
      "  warnings.warn(ECOS_DEPRECATION_MSG, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "Weak.virtual_labels(p=None, optimize = False, convex = True) #This is to create Virtual Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.include_weak(Weak.z)\n",
    "#Data.include_virtual(Weak.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader,testloader = Data.get_dataloader(weak_labels='weak')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([  3, 339, 531, 353, 512,  19,  27, 431, 303, 167,  60, 448,   0, 547,\n",
      "        521, 261], dtype=torch.int32)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "examples = next(iter(trainloader))\n",
    "for k in examples:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1796228 , 0.06898198, 0.12434957, ..., 0.09171096, 0.10871832,\n",
       "        0.07654192],\n",
       "       [0.14023764, 0.15947086, 0.11958131, ..., 0.08669024, 0.16870691,\n",
       "        0.12970188],\n",
       "       [0.14231886, 0.05226663, 0.05998836, ..., 0.06022358, 0.16111562,\n",
       "        0.06627856],\n",
       "       ...,\n",
       "       [0.07704287, 0.0965763 , 0.13342503, ..., 0.10547777, 0.04821152,\n",
       "        0.20792012],\n",
       "       [0.01375156, 0.1331928 , 0.13435628, ..., 0.20184827, 0.1220006 ,\n",
       "        0.13111119],\n",
       "       [0.09522949, 0.09678306, 0.03010278, ..., 0.15785421, 0.15120496,\n",
       "        0.00619227]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weak.V_matrix(Data.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(Data\u001b[38;5;241m.\u001b[39mnum_features,[Data\u001b[38;5;241m.\u001b[39mnum_features],Data\u001b[38;5;241m.\u001b[39mnum_classes, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, bn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, activation \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(mlp\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m mlp, results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43msound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m overall_results[i] \u001b[38;5;241m=\u001b[39m results\n\u001b[0;32m     10\u001b[0m overall_models[i] \u001b[38;5;241m=\u001b[39m mlp  \n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\vscode_projects\\Learning_from_weak_labels\\utils\\trainig_testing.py:88\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, trainloader, testloader, optimizer, loss_fn, num_epochs, sound)\u001b[0m\n\u001b[0;32m     86\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, vl, ind)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\vscode_projects\\Learning_from_weak_labels\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\vscode_projects\\Learning_from_weak_labels\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danibacaicoa\\vscode_projects\\Learning_from_weak_labels\\utils\\losses.py:253\u001b[0m, in \u001b[0;36mFBLoss.forward\u001b[1;34m(self, out, z)\u001b[0m\n\u001b[0;32m    251\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(out)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m#Loss L(z,f) = z'L(f) = z'V'phi(VMf)\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV\u001b[38;5;241m.\u001b[39mT\u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVM\u001b[49m\u001b[38;5;129;43m@p\u001b[39;49m))[z]\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m L\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for @: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "\n",
    "loss = losses.FBLoss(Weak.M,Weak.V)\n",
    "overall_results = {}\n",
    "overall_models = {}\n",
    "epochs = 5\n",
    "for i in range(2):\n",
    "    mlp = MLP(Data.num_features,[Data.num_features],Data.num_classes, dropout_p=0.5, bn=True, activation =  'gelu')\n",
    "    optim = torch.optim.Adam(mlp.parameters(),lr=1e-2)\n",
    "    mlp, results = train_and_evaluate(mlp,trainloader,testloader,optimizer=optim,loss_fn=loss,num_epochs=epochs,sound=1)\n",
    "    overall_results[i] = results\n",
    "    overall_models[i] = mlp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TM = torch.tensor(Weak.M)\n",
    "TV = torch.tensor(Weak.V)\n",
    "TVM = TV@TM\n",
    "TVM.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = next(iter(trainloader))\n",
    "yhat = mlp(examples[0])\n",
    "\n",
    "soft= torch.nn.Softmax(dim=1)\n",
    "p = soft(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: double != float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m TV\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[43mTVM\u001b[49m\u001b[38;5;129;43m@p\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: double != float"
     ]
    }
   ],
   "source": [
    "TV.T\n",
    "torch.log(TVM@p.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FBLoss(nn.Module):\n",
    "    def __init__(self, M, V):\n",
    "        super(FBLoss, self).__init__()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.M = torch.tensor(M)\n",
    "        self.V = torch.tensor(V)\n",
    "        self.VM = V@M\n",
    "        \n",
    "    def forward(self,out,z):\n",
    "        p = self.softmax(out)\n",
    "        #Loss L(z,f) = z'L(f) = z'V'phi(VMf)\n",
    "        L = -(self.V.T@torch.log(self.VM@p))[z]\n",
    "\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "Results_mnist_ce = [overall_results,overall_models]\n",
    "f = open(\"Experimental_results/EM.pkl\",\"wb\")\n",
    "pickle.dump(Results_mnist_ce,f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tr_loss = np.array(overall_results[0]['train_loss'])\n",
    "tr_acc = np.array(overall_results[0]['train_acc'])\n",
    "te_acc = np.array(overall_results[0]['test_acc'])\n",
    "for i in range(1):\n",
    "    tr_loss = np.vstack((tr_loss,overall_results[i+1]['train_loss']))\n",
    "    tr_acc = np.vstack((tr_acc,overall_results[i+1]['train_acc']))\n",
    "    te_acc = np.vstack((te_acc,overall_results[i+1]['test_acc']))\n",
    "plt.plot(np.mean(tr_loss,0))\n",
    "plt.plot(np.mean(tr_loss,0)+np.std(tr_loss,0),'g--')\n",
    "plt.plot(np.mean(tr_loss,0)-np.std(tr_loss,0),'g--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(te_acc,0))\n",
    "plt.plot(np.mean(te_acc,0)+np.std(te_acc,0),'g--')\n",
    "plt.plot(np.mean(te_acc,0)-np.std(te_acc,0),'g--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(tr_acc,0))\n",
    "plt.plot(np.mean(tr_acc,0)+np.std(tr_acc,0),'g--')\n",
    "plt.plot(np.mean(tr_acc,0)-np.std(tr_acc,0),'g--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
