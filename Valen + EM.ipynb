{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets.openml_datasets import OpenML_Dataset\n",
    "from datasets.torch_datasets import Torch_Dataset\n",
    "from utils.weakener import Weakener\n",
    "from models.general_model import MLP\n",
    "from utils.losses import PartialLoss,LBLoss,EMLoss,OSLCELoss,OSLBrierLoss,CELoss\n",
    "from utils.trainig_testing import train_model,evaluate_model,train_and_evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "weakening = 'random' #for possible ['random','feature']\n",
    "\n",
    "Data = Torch_Dataset(dataset, batch_size=256)\n",
    "Weak = Weakener(Data.num_classes)\n",
    "Weak.generate_M('pll',pll_p=0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(tensor([678, 841, 560,  ..., 116, 736, 584], dtype=torch.int32),\n tensor([[1., 0., 1.,  ..., 0., 0., 1.],\n         [1., 1., 0.,  ..., 1., 0., 0.],\n         [1., 0., 0.,  ..., 0., 1., 1.],\n         ...,\n         [0., 0., 0.,  ..., 1., 0., 0.],\n         [1., 0., 1.,  ..., 0., 1., 1.],\n         [1., 0., 0.,  ..., 0., 1., 1.]], dtype=torch.float64))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X,train_y,test_X,test_y =  Data.get_data()\n",
    "print(train_X.shape)\n",
    "Weak.generate_weak(train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "mlp_feature = MLP(Data.num_features, [Data.num_features, Data.num_features, Data.num_features], Data.num_classes, dropout_p = 0.0, bn = False, seed = 1,\n",
    "                  layer_init = lambda x: nn.init.kaiming_uniform_(x, a=math.sqrt(5)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['layers.0.weight', 'layers.0.bias', 'layers.1.weight', 'layers.1.bias', 'layers.2.weight', 'layers.2.bias', 'layers.3.weight', 'layers.3.bias'])\n",
      "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(mlp_feature.state_dict().keys())\n",
    "\n",
    "valen_weights = torch.load('results/mnist_random')\n",
    "print(valen_weights.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "tensor([[ 0.0172, -0.0161, -0.0075,  ...,  0.0059, -0.0047,  0.0017],\n",
      "        [-0.0267, -0.0219, -0.0302,  ..., -0.0029,  0.0012, -0.0163],\n",
      "        [ 0.0275, -0.0093, -0.0333,  ..., -0.0138,  0.0055, -0.0328],\n",
      "        ...,\n",
      "        [-0.0015, -0.0159, -0.0231,  ..., -0.0109,  0.0195, -0.0093],\n",
      "        [ 0.0065,  0.0274, -0.0327,  ...,  0.0053,  0.0243,  0.0175],\n",
      "        [-0.0229, -0.0293,  0.0004,  ..., -0.0110,  0.0073, -0.0074]])\n",
      "layers.0.weight\n",
      "tensor([[ 0.0035,  0.0052,  0.0204,  ...,  0.0049,  0.0135, -0.0151],\n",
      "        [ 0.0047, -0.0078,  0.0343,  ..., -0.0142, -0.0098,  0.0058],\n",
      "        [-0.0316, -0.0248, -0.0252,  ..., -0.0233, -0.0335,  0.0179],\n",
      "        ...,\n",
      "        [-0.0104,  0.0225, -0.0054,  ..., -0.0116, -0.0109, -0.0114],\n",
      "        [-0.0007, -0.0176, -0.0028,  ..., -0.0159,  0.0323,  0.0165],\n",
      "        [ 0.0004, -0.0072,  0.0057,  ...,  0.0117, -0.0161, -0.0064]])\n"
     ]
    }
   ],
   "source": [
    "# We see that the mlp parameters and the weights taken from valen are initially different\n",
    "w = 'layers.0.weight'\n",
    "w1 = w.split('.')\n",
    "name = 'fc'+str(int(w1[1])+1)+'.'+w1[2]\n",
    "print(name)\n",
    "print(valen_weights[name])\n",
    "print(w)\n",
    "print(mlp_feature.state_dict()[w])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def weight_allocation(net,weights):\n",
    "    for name, param in net.named_parameters():\n",
    "        w = name.split('.')\n",
    "        w_name = 'fc'+str(int(w[1])+1)+'.'+w[2]\n",
    "        param.data = weights[w_name]\n",
    "    return net\n",
    "mlp_feature = weight_allocation(mlp_feature,valen_weights)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight\n",
      "tensor([[ 0.0172, -0.0161, -0.0075,  ...,  0.0059, -0.0047,  0.0017],\n",
      "        [-0.0267, -0.0219, -0.0302,  ..., -0.0029,  0.0012, -0.0163],\n",
      "        [ 0.0275, -0.0093, -0.0333,  ..., -0.0138,  0.0055, -0.0328],\n",
      "        ...,\n",
      "        [-0.0015, -0.0159, -0.0231,  ..., -0.0109,  0.0195, -0.0093],\n",
      "        [ 0.0065,  0.0274, -0.0327,  ...,  0.0053,  0.0243,  0.0175],\n",
      "        [-0.0229, -0.0293,  0.0004,  ..., -0.0110,  0.0073, -0.0074]])\n",
      "layers.0.weight\n",
      "tensor([[ 0.0172, -0.0161, -0.0075,  ...,  0.0059, -0.0047,  0.0017],\n",
      "        [-0.0267, -0.0219, -0.0302,  ..., -0.0029,  0.0012, -0.0163],\n",
      "        [ 0.0275, -0.0093, -0.0333,  ..., -0.0138,  0.0055, -0.0328],\n",
      "        ...,\n",
      "        [-0.0015, -0.0159, -0.0231,  ..., -0.0109,  0.0195, -0.0093],\n",
      "        [ 0.0065,  0.0274, -0.0327,  ...,  0.0053,  0.0243,  0.0175],\n",
      "        [-0.0229, -0.0293,  0.0004,  ..., -0.0110,  0.0073, -0.0074]])\n"
     ]
    }
   ],
   "source": [
    "w = 'layers.0.weight'\n",
    "w1 = w.split('.')\n",
    "name = 'fc'+str(int(w1[1])+1)+'.'+w1[2]\n",
    "print(name)\n",
    "print(valen_weights[name])\n",
    "print(w)\n",
    "print(mlp_feature.state_dict()[w])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "z, w = Weak.generate_weak(train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "Data.include_weak(Weak.z)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "trainloader, testloader = Data.get_dataloader()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 0.9358\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(0.9358, dtype=torch.float64)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(mlp_feature,testloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10: Train Loss: nan, Train Acc: 0.1027, Test Acc: 0.0980\n",
      "Epoch 2/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 3/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 4/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n",
      "Epoch 5/10: Train Loss: nan, Train Acc: 0.0987, Test Acc: 0.0980\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSGD(mlp_feature\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m, weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m,momentum\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m,nesterov\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m )\n\u001B[0;32m      4\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m EMLoss(Weak\u001B[38;5;241m.\u001B[39mM)\n\u001B[1;32m----> 6\u001B[0m mlp_feature,results \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmlp_feature\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtestloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\utils\\trainig_testing.py:94\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[1;34m(model, trainloader, testloader, optimizer, loss_fn, num_epochs)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, vl, targets, ind \u001B[38;5;129;01min\u001B[39;00m trainloader:\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;66;03m#print(inputs, vl, targets, ind)\u001B[39;00m\n\u001B[0;32m     93\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 94\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(inspect\u001B[38;5;241m.\u001B[39mgetfullargspec(loss_fn\u001B[38;5;241m.\u001B[39mforward)\u001B[38;5;241m.\u001B[39margs)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m3\u001B[39m:\n\u001B[0;32m     96\u001B[0m         loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, vl, ind)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\Learning_from_weak_labels\\models\\general_model.py:47\u001B[0m, in \u001B[0;36mMLP.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m---> 47\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn:\n\u001B[0;32m     49\u001B[0m             x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_norms[i](x)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\Weak_Label_Learning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#mlp = MLP(Data.num_features,[Data.num_features,Data.num_features,Data.num_features],Data.num_classes,dropout_p=0.3)\n",
    "#optimizer = torch.optim.Adam(mlp_feature.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(mlp_feature.parameters(), lr=1e-2, weight_decay=1e-4,momentum=0.9,nesterov=True )\n",
    "loss_fn = EMLoss(Weak.M)\n",
    "\n",
    "mlp_feature,results = train_and_evaluate(mlp_feature,trainloader,testloader,optimizer,loss_fn,num_epochs=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
